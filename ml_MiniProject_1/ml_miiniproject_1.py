# -*- coding: utf-8 -*-
"""ML_MIiniProject_1.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1NwZ3BcL3-qJCNFXE9iJAdm-rj0CTM8ec

# 1-1
"""

#توضیحی در گزارش

"""# 1-2 Generate Dataset"""

from sklearn.datasets import load_breast_cancer, make_classification, make_blobs, make_circles, load_digits
import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
import plotly.express as px
from sklearn.decomposition import PCA
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression, SGDClassifier
from sklearn.preprocessing import StandardScaler, MinMaxScaler
from mpl_toolkits.mplot3d import Axes3D
from mlxtend.plotting import plot_decision_regions

# Generate dataset
X, y = make_classification(n_samples=1000,
                           n_features=3,
                           n_classes=4,
                           n_redundant=0,
                           n_clusters_per_class=1,
                           class_sep=2,
                           #n_informative = 3,
                           random_state=4)
print(X.shape, y.shape)

# Create a DataFrame from the generated dataset
df = pd.DataFrame(X, columns=['Feature 1', 'Feature 2', 'Feature 3'])
df['Target'] = y

# Plot the dataset using Plotly Express
fig = px.scatter_3d(df, x='Feature 1', y='Feature 2', z='Feature 3', color='Target', title='make_classification Dataset')
fig.show()

# Perform PCA for dimensionality reduction
pca = PCA(n_components=2)
X_pca = pca.fit_transform(X)

# Plot the data
plt.figure(figsize=(8, 6))

# Scatter plot
plt.scatter(X_pca[:, 0], X_pca[:, 1], c=y, cmap=plt.cm.rainbow)
plt.xlabel('Principal Component 1')
plt.ylabel('Principal Component 2')
plt.title('2D PCA Plot of the Data')

plt.colorbar(label='Class')

plt.show()

"""# 1-3 Train Model

## LogisticRegression
"""

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=4)
X_train.shape, y_train.shape, X_test.shape, y_test.shape

# Compute the range for each feature
feature_ranges = [(np.min(X_train[:, i]), np.max(X_train[:, i])) for i in range(X_train.shape[1])]

# Print the range for each feature
for i, (min_val, max_val) in enumerate(feature_ranges):
    print(f"Feature {i+1}: Range = ({min_val}, {max_val})")

scaler = MinMaxScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

C_values = np.linspace(0.01, 1, 50)
solver_list = ['lbfgs', 'liblinear', 'newton-cg', 'newton-cholesky', 'sag', 'saga']
best_result = 0
for C in C_values:
  for solver in solver_list:
    for i in np.linspace(1e-5, 1e-3, 50):
      LR_model = LogisticRegression(penalty='l2',
                              dual=False,
                              tol = i,
                              solver = solver,
                              max_iter=1000,
                              C = C,
                              random_state=4)
      LR_model.fit(X_train, y_train)
      result = LR_model.score(X_train, y_train)
      if result > best_result:
        best_result = result
        print (f"best result is {best_result} with solver: {solver} , C= {C}  and tol= {i}")

best_result = 0
for C in C_values:
  for solver in solver_list:
    for i in np.linspace(1e-5, 1e-3, 50):
      LR_model = LogisticRegression(penalty='l2',
                              dual=False,
                              tol = i,
                              solver = solver,
                              max_iter=1000,
                              C = C,
                              random_state=4)
      LR_model.fit(X_train_scaled, y_train)
      result = LR_model.score(X_train_scaled, y_train)
      if result > best_result:
        best_result = result
        print (f"best result is {best_result} with solver: {solver} , C= {C}  and tol= {i}")

LR_model = LogisticRegression(penalty='l2',
                              dual=False,
                              tol = 1e-05,
                              solver='lbfgs',
                              max_iter=1000,
                              C=0.9393877551020408,
                              random_state=4)
LR_model.fit(X_train, y_train)
LR_model.predict(X_test), y_test

LR_model.fit(X_train, y_train)
LR_model.predict(X_test), y_test

print("LR_model train score = ", LR_model.score(X_train, y_train))
print("LR_model predict score = ", LR_model.score(X_test, y_test))

"""## SGDClassifier"""

loss_list = ['hinge', 'log_loss', 'squared_hinge', 'squared_error', 'huber']
learning_rate = ['optimal', 'adaptive']
best_result = 0
for loss in loss_list:
  for lr in learning_rate:
    for eta in np.linspace(1e-3, 30, 50):
      SGD_model = SGDClassifier(loss=loss,
                                max_iter =2000,
                                tol = 1e-3,
                                eta0 = eta,
                                learning_rate = lr,
                                random_state=4)
      SGD_model.fit(X_train, y_train)
      result = SGD_model.score(X_train, y_train)
      if result > best_result:
        best_result = result
        print (f"best result is {best_result} with loss: {loss} , learning rate= {lr} and eta0= {eta}")

SGD_model = SGDClassifier(loss='hinge' ,
                          max_iter =2000,
                          tol = 1e-3,
                          eta0 = 6.735469387755102,
                          learning_rate = 'adaptive' ,
                          random_state=4)
SGD_model.fit(X_train, y_train)
SGD_model.predict(X_test), y_test

SGD_model.fit(X_train, y_train)
SGD_model.predict(X_test), y_test

print("SGD_model train score = ", SGD_model.score(X_train, y_train))
print("SGD_model predict score = ", SGD_model.score(X_test, y_test))

"""# 1-4 Plot Decision Boundary"""

pca = PCA(n_components=2)
X_pca = pca.fit_transform(X)
X_train_pca = pca.transform(X_train)
X_test_pca = pca.transform(X_test)

X.shape

# Fit logistic regression model
LR_model_pca = LogisticRegression(penalty='l2',
                              dual=False,
                              tol=1e-05,
                              solver='lbfgs',
                              max_iter=1000,
                              C=0.9393877551020408,
                              random_state=4)
LR_model_pca.fit(X_train_pca, y_train)
LR_model_pca.predict(X_test_pca)
print(LR_model_pca.score(X_train_pca, y_train))
print(LR_model_pca.score(X_test_pca, y_test))

X_pca.shape

x1_min, x1_max = X_pca[:, 0].min() - 1, X_pca[:, 0].max() + 1
x2_min, x2_max = X_pca[:, 1].min() - 1, X_pca[:, 1].max() + 1
xx1, xx2 = np.meshgrid(np.linspace(x1_min, x1_max, 500),
                       np.linspace(x2_min, x2_max, 500))

Z = LR_model_pca.predict(np.c_[xx1.ravel(), xx2.ravel()])
Z = Z.reshape(xx1.shape)

plt.contourf(xx1, xx2, Z, alpha=0.4)

predicted_classes = LR_model_pca.predict(X_pca)
misclassified_indices = np.where(predicted_classes != y)[0]
misclassified_points = X_pca[misclassified_indices]

plt.xlabel('Feature 1')
plt.ylabel('Feature 2')
plt.title('Decision Boundaries with Misclassified Points')
plt.legend()
plt.show()

from mlxtend.plotting import plot_decision_regions
plot_decision_regions(X_pca, y, clf=LR_model_pca)

"""# 1-5 draw data"""

!pip install drawdata

from drawdata import ScatterWidget

widget = ScatterWidget()
widget

# Get the drawn data as a list of dictionaries
widget.data
# Get the drawn data as a dataframe
draedata_df = widget.data_as_pandas

draedata_df.replace({'a': 1, 'b': 2, 'c': 3, 'd': 4}, inplace=True)
draedata_df

X = draedata_df.iloc[:, :2].values
y = draedata_df.iloc[:, 3].values

import plotly.express as px
fig = px.scatter(draedata_df, x="x", y="y", color="label", size='x')
fig.show()

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=4)
X_train.shape, y_train.shape, X_test.shape, y_test.shape

"""## LogisticRegression"""

LR_model = LogisticRegression(penalty='l2',
                              dual=False,
                              tol = 1e-05,
                              solver='lbfgs',
                              max_iter=1000,
                              C=1,
                              random_state=4)
LR_model.fit(X_train, y_train)
LR_model.predict(X_test)
print("LR_model train score = ", LR_model.score(X_train, y_train))
print("LR_model test score = ", LR_model.score(X_test, y_test))

print("LR_model train score = ", LR_model.score(X_train, y_train))
print("LR_model test score = ", LR_model.score(X_test, y_test))

"""## SGDClassifier"""

loss_list = ['hinge', 'log_loss', 'squared_hinge', 'squared_error', 'huber']
learning_rate = ['optimal', 'adaptive']
best_result = 0
for loss in loss_list:
  for lr in learning_rate:
    for eta in np.linspace(1e-3, 30, 50):
      SGD_model = SGDClassifier(loss=loss,
                                max_iter =2000,
                                tol = 1e-3,
                                eta0 = eta,
                                learning_rate = lr,
                                random_state=4)
      SGD_model.fit(X_train, y_train)
      result = SGD_model.score(X_train, y_train)
      if result > best_result:
        best_result = result
        print (f"best result is {best_result} with loss: {loss} , learning rate= {lr} and eta0= {eta}")

SGD_model = SGDClassifier(loss='squared_hinge' ,
                          max_iter =2000,
                          tol = 1e-3,
                          eta0 = 0.001,
                          learning_rate = 'adaptive' ,
                          random_state=4)
SGD_model.fit(X_train, y_train)
SGD_model.predict(X_test), y_test

SGD_model.predict(X_test)
print("SGD_model train score = ", SGD_model.score(X_train, y_train))
print("SGD_model test score = ", SGD_model.score(X_test, y_test))

"""## Plot Dicision Boundry"""

from mlxtend.plotting import plot_decision_regions
plot_decision_regions(X, y, clf=LR_model)

"""# Problem 2

## 2-1
"""

#مراجعه به صفحه دیتاست و دانلود

"""##2-2

### 2-2-a
"""

# Commented out IPython magic to ensure Python compatibility.
# %cd /content

!gdown 14JXJeFLn-V1KSM5yNAiwokaX1F9P3haS
!gdown 1oqLyBpuEfXOpYIZ-aY_gzznCPsBz1pGp

from scipy.io import loadmat
import numpy as np
from scipy.stats import skew, kurtosis
from sklearn.utils import shuffle
from sklearn.model_selection import train_test_split

# Load the .mat file
normal_data = loadmat('/content/97.mat')
fault_data = loadmat('/content/105.mat')

print(normal_data.keys())
print(fault_data.keys())

normal_data_variable = normal_data['X097_DE_time']
print(type(normal_data_variable))  # Print the type of the variable
print(normal_data_variable.shape)  # Print the shape of the variable (if it's a numpy array)

fault_data_variable = fault_data['X105_DE_time']
print(type(fault_data_variable))  # Print the type of the variable
print(fault_data_variable.shape)  # Print the shape of the variable (if it's a numpy array)

# Extract 10 samples, each containing 5 data points
n_samples = 120
n_data = 220
ext_normal_data = normal_data_variable[:n_samples * n_data, 0]
ext_fault_data = fault_data_variable[:n_samples * n_data, 0]

print(ext_normal_data.shape)
print(ext_fault_data.shape)

# Reshape the extracted data to have 10 rows and 5 columns
ext_normal_data = ext_normal_data.reshape(n_samples, n_data)
ext_fault_data = ext_fault_data.reshape(n_samples, n_data)

print(ext_normal_data.shape)
print(ext_fault_data.shape)

"""### 2-2-b"""

ext_normal_data[:, 0]

#Normal data feature extraction
normal_standard_deviations = np.std(ext_normal_data, axis=1)
normal_skewnesses = skew(ext_normal_data, axis=1)
normal_kurtoses = kurtosis(ext_normal_data, axis=1)
normal_peak_to_peaks = np.ptp(ext_normal_data, axis=1)
normal_root_mean_squares = np.sqrt(np.mean(np.square(ext_normal_data), axis=1))
normal_means = np.mean(ext_normal_data, axis=1)
normal_absolute_means = np.mean(np.abs(ext_normal_data), axis=1)
normal_peaks = np.max(ext_normal_data, axis=1)

print("Standard Deviations:", normal_standard_deviations.shape)
print("Skewnesses:", normal_skewnesses.shape)
print("Kurtoses:", normal_kurtoses.shape)
print("Peak to Peaks:", normal_peak_to_peaks.shape)
print("Root Mean Squares:", normal_root_mean_squares.shape)
print("Means:", normal_means.shape)
print("Absolute Means:", normal_absolute_means.shape)
print("Peaks:", normal_peaks.shape)

normal_ext_feature_dataset = np.column_stack((normal_standard_deviations, normal_skewnesses, normal_kurtoses, normal_peak_to_peaks,
                              normal_root_mean_squares, normal_means, normal_absolute_means, normal_peaks))


normal_ext_feature_dataset = np.hstack((normal_ext_feature_dataset, np.zeros((len(normal_ext_feature_dataset), 1))))
print("final normal dataset: ", normal_ext_feature_dataset.shape)

#fault data feature extraction
fault_standard_deviations = np.std(ext_fault_data, axis=1)
fault_skewnesses = skew(ext_fault_data, axis=1)
fault_kurtoses = kurtosis(ext_fault_data, axis=1)
fault_peak_to_peaks = np.ptp(ext_fault_data, axis=1)
fault_root_mean_squares = np.sqrt(np.mean(np.square(ext_fault_data), axis=1))
fault_means = np.mean(ext_fault_data, axis=1)
fault_absolute_means = np.mean(np.abs(ext_fault_data), axis=1)
fault_peaks = np.max(ext_fault_data, axis=1)

print("Standard Deviations:", fault_standard_deviations.shape)
print("Skewnesses:", fault_skewnesses.shape)
print("Kurtoses:", fault_kurtoses.shape)
print("Peak to Peaks:", fault_peak_to_peaks.shape)
print("Root Mean Squares:", fault_root_mean_squares.shape)
print("Means:", fault_means.shape)
print("Absolute Means:", fault_absolute_means.shape)
print("Peaks:", fault_peaks.shape)

fault_ext_feature_dataset = np.column_stack((fault_standard_deviations, fault_skewnesses, fault_kurtoses, fault_peak_to_peaks,
                              fault_root_mean_squares, fault_means, fault_absolute_means, fault_peaks))

fault_ext_feature_dataset = np.hstack((fault_ext_feature_dataset, np.ones((len(fault_ext_feature_dataset), 1))))
print("final fault dataset: ", fault_ext_feature_dataset.shape)

final_data = np.concatenate((normal_ext_feature_dataset, fault_ext_feature_dataset), axis=0)
final_data.shape

"""### 2-2-c"""

shuffled_final_data = shuffle(final_data)
shuffled_final_data.shape

X = shuffled_final_data[:, :-1]
y = shuffled_final_data[:, -1]
print(X.shape)
print(y.shape)

x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.1)
x_train.shape, x_test.shape, y_train.shape, y_test.shape

# Reshape y_train and y_test
y_train = np.reshape(y_train, (-1, 1))
y_test = np.reshape(y_test, (-1, 1))
x_train.shape, x_test.shape, y_train.shape, y_test.shape

"""### 2-2-d"""

from sklearn.preprocessing import MinMaxScaler, StandardScaler

# Initialize MinMaxScaler
scaler = MinMaxScaler()

# Fit the scaler on the training data
scaler.fit(x_train)

# Transform the training and testing data
x_train = scaler.transform(x_train)
x_test = scaler.transform(x_test)

"""## 2-3"""

def sigmoid(x):
  return 1 / (1 + np.exp(-x))

def logestic_regression(x, w):
  y_hat = sigmoid(x @ w)
  return y_hat

def bce(y, y_hat):
  loss = -(np.mean(y*np.log(y_hat) + (1-y)*np.log(1-y_hat)))
  return loss

def gradient(x, y, y_hat):
    grads = (x.T @ (y_hat - y)) / len(y)
    return grads

def gradient_descent(w, eta, grads):
    w -= eta*grads
    return w

def accuracy(y, y_hat):
  acc = np.sum(y == np.round(y_hat))/len(y)
  return acc

x_train = np.hstack((np.ones((len(x_train), 1)), x_train))

x_train.shape

m = 8 #num of features
w = np.random.randn(m+1, 1)
print(w.shape)

num_epoch = 1000
eta = 0.01

error_hist = []

for epoch in range(num_epoch):
  y_hat = logestic_regression(x_train, w)
  loss = bce(y_train, y_hat)
  error_hist.append(loss)
  grads = gradient(x_train, y_train, y_hat)
  w = gradient_descent(w, eta, grads)
  if epoch % 50 == 0:
    print(f'Epoch = {epoch+1}, \t loss = {loss:.4},\t w={w.T[0, 0]}')

import matplotlib.pyplot as plt
plt.plot(error_hist)

acc_train = accuracy(y_train, y_hat)
print("train accuracy = ", acc_train)

"""### Test"""

x_test = np.hstack((np.ones((len(x_test), 1)), x_test))

#x_test = np.delete(x_test, 0, axis=1)

x_test.shape

y_hat_test = logestic_regression(x_test, w)
y_hat_test, y_test

acc_test = accuracy(y_test, y_hat_test)

from sklearn.metrics import f1_score
f1score = f1_score(y_test, np.round(y_hat_test))
print("test accuracy:", acc_test)
print("test f1score:", f1score)

"""## 2-4"""

from sklearn.linear_model import LogisticRegression, SGDClassifier
y_train = y_train.ravel()
SGD_model = SGDClassifier(loss='log_loss' ,
                          max_iter =1000,
                          tol = 1e-3,
                          eta0 = 0.001,
                          learning_rate = 'adaptive' ,
                          random_state=4)
SGD_model.fit(x_train, y_train)
SGD_model.predict(x_test), y_test
print("LR_model train score = ", SGD_model.score(x_train, y_train))
print("LR_model test score = ", SGD_model.score(x_test, y_test))

from sklearn.linear_model import LogisticRegression, SGDClassifier
y_train = y_train.ravel()
LR_model = LogisticRegression(penalty='l2',
                              dual=False,
                              tol = 1e-05,
                              solver='sag',
                              max_iter=1000,
                              C=1,
                              random_state=4)
LR_model.fit(x_train, y_train)
LR_model.predict(x_test)
print("LR_model train score = ", LR_model.score(x_train, y_train))
print("LR_model test score = ", LR_model.score(x_test, y_test))

from sklearn.metrics import log_loss
train_loss = []
# Train the classifier and collect loss values
epochs = 100
for epoch in range(epochs):
    SGD_model.partial_fit(x_train, y_train, classes=np.unique(y_train))
    loss = log_loss(y_train, SGD_model.predict_proba(x_train))
    train_loss.append(loss)

# Plot the training loss curve
plt.plot(range(1, epochs + 1), train_loss, label='Training Loss')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.title('Training Loss Curve')
plt.legend()
plt.show()

"""##2-5"""

#بصورت توضیحی در گزارش

"""# Problem 3

## 3-1
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from matplotlib import pyplot as plt
import numpy as np
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error
import statsmodels.api as sm

# Commented out IPython magic to ensure Python compatibility.
# %cd /content
!gdown 1fXFPECCGZ7-Kc8wXw8VhfGrsASlAOdAz

df = pd.read_csv("/content/weatherHistory.csv")
df

# Exclude the last column from the DataFrame
feature_df = df.iloc[:, [3,4,5,6,7,8,10]]

# Calculate correlation matrix
corr_matrix = feature_df.corr()

# Create heatmap using seaborn with a colormap from -1 to 1
plt.figure(figsize=(7, 5))  # Adjust the figure size as needed
sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', linewidths=0.5, annot_kws={"size": 8}, fmt='.3f',
            yticklabels=corr_matrix.columns, vmin=-1, vmax=1)  # Set vmin and vmax

# Adjust font size of annotations
plt.xticks(fontsize=12)
plt.yticks(fontsize=12)

# Adjust margins of PDF file
plt.savefig('PIcS1.pdf', bbox_inches='tight')

Humidity = df['Humidity'].values
Temperature = df["Temperature (C)"].values
Apparent_Temperature = df["Apparent Temperature (C)"].values
Visibility = df["Visibility (km)"].values
print(Humidity.shape, Temperature.shape, Apparent_Temperature.shape, Visibility.shape)

Humidity = np.reshape(Humidity, (-1, 1))
Temperature = np.reshape(Temperature, (-1, 1))
Apparent_Temperature = np.reshape(Apparent_Temperature, (-1, 1))
Visibility = np.reshape(Visibility, (-1, 1))
print(Humidity.shape, Temperature.shape, Apparent_Temperature.shape, Visibility.shape)

# Plot histogram for 'Temperature (C)'
plt.figure(figsize=(12, 2.3))
plt.subplot(1, 4, 1)
feature_df['Temperature (C)'].plot(kind='hist', bins=20, title='Temperature')
plt.gca().spines[['top', 'right']].set_visible(False)

# Plot histogram for 'Apparent Temperature (C)'
plt.subplot(1, 4, 2)
feature_df['Apparent Temperature (C)'].plot(kind='hist', bins=20, title='Apparent Temperature')
plt.gca().spines[['top', 'right']].set_visible(False)

# Plot histogram for 'Humidity'
plt.subplot(1, 4, 3)
feature_df['Humidity'].plot(kind='hist', bins=20, title='Humidity')
plt.gca().spines[['top', 'right']].set_visible(False)

# Plot histogram for 'Visibility'
plt.subplot(1, 4, 4)
feature_df['Visibility (km)'].plot(kind='hist', bins=20, title='Visibility')
plt.gca().spines[['top', 'right']].set_visible(False)

# Adjust layout
plt.tight_layout()

# Show plot
plt.show()

sns.pairplot(feature_df)

"""## 3-2

### LS
"""

class LinearRegressionLS:
    def __init__(self):
        self.coefficients = None

    def fit(self, X, y):
        # Add a column of ones to account for the intercept term
        X = np.column_stack((np.ones(len(X)), X))

        # Compute the coefficients using the least squares method
        self.coefficients = np.linalg.inv(X.T.dot(X)).dot(X.T).dot(y)

    def predict(self, X):
        # Add a column of ones to account for the intercept term
        X = np.column_stack((np.ones(len(X)), X))

        # Predict the target variable
        return X.dot(self.coefficients)

Temperature.shape

type(Humidity)

# X = Humidity
# X = np.concatenate((Humidity, Temperature), axis=1)
# y = Apparent_Temperature
X = np.concatenate((Humidity, Temperature, Visibility), axis=1)
y = Apparent_Temperature

# Split the dataset into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=4)
print(X_train.shape, X_test.shape, y_train.shape, y_test.shape)
print()


# Initialize and fit the linear regression model using least squares
model = LinearRegressionLS()
model.fit(X_train, y_train)

# Make predictions on the testing set
y_pred = model.predict(X_test)

# Calculate Mean Squared Error
mse = mean_squared_error(y_test, y_pred)
print("Mean Squared Error:", mse)

import matplotlib.pyplot as plt

# Plot actual vs predicted values
plt.figure(figsize=(10, 6))
plt.scatter(y_test, y_pred, color='blue')
plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], color='red', linestyle='--')
plt.xlabel('Actual Values')
plt.ylabel('Predicted Values')
plt.title('Actual vs Predicted Values')
plt.grid(True)
plt.show()

"""### RLS"""

class RecursiveLeastSquares:
    def __init__(self, n_features, forgetting_factor=0.99):
        self.n_features = n_features
        self.forgetting_factor = forgetting_factor
        self.theta = np.zeros((n_features, 1))  # Initialize model parameters
        self.P = np.eye(n_features)  # Initialize covariance matrix

    def fit(self, X, y):
        errors = []
        for i in range(len(X)):
            x_i = X[i].reshape(-1, 1)
            y_i = y[i]

            # Predict
            y_pred = np.dot(x_i.T, self.theta)

            # Update
            error = y_i - y_pred
            errors.append(error)
            K = np.dot(self.P, x_i) / (self.forgetting_factor + np.dot(np.dot(x_i.T, self.P), x_i))
            self.theta = self.theta + np.dot(K, error)
            self.P = (1 / self.forgetting_factor) * (self.P - np.dot(K, np.dot(x_i.T, self.P)))

        return errors

    def predict(self, X):
        return np.dot(X, self.theta)

# Split the dataset into features (X) and target variable (y)
# X = np.concatenate((Humidity, Temperature), axis=1)
# y = Apparent_Temperature
X = np.concatenate((Humidity, Visibility, Temperature), axis=1)
y = Apparent_Temperature

# Split the dataset into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Initialize and fit the RLS model
rls = RecursiveLeastSquares(n_features=X_train.shape[1], forgetting_factor=0.99)
errors = rls.fit(X_train, y_train)

# Make predictions
y_pred = rls.predict(X_test)

# Calculate Mean Squared Error
mse = np.mean(np.array(errors)**2)
print("Mean Squared Error:", mse)
print()

# Plot actual vs predicted values
plt.figure(figsize=(9, 5))
plt.scatter(y_test, y_pred, color='blue')
plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], color='red', linestyle='--')
plt.xlabel('Actual Values')
plt.ylabel('Predicted Values')
plt.title('Actual vs Predicted Values (Boston Housing dataset)')
plt.grid(True)
plt.show()

mse_list = []
for ff in np.linspace(0.5, 0.99, 30):
    # Initialize and fit the RLS model
    rls = RecursiveLeastSquares(n_features=X_train.shape[1], forgetting_factor = ff)
    errors = rls.fit(X_train, y_train)

    # Make predictions
    y_pred = rls.predict(X_test)

    # Calculate Mean Squared Error
    mse = np.mean(np.array(errors)**2)
    mse_list.append(mse)
    # print("Mean Squared Error:", mse, "           Forgetting Factor:", ff)
    print("Mean Squared Error: {:.10f}      Forgetting Factor: {:.10f}".format(mse, ff))

plt.plot(mse_list)

# X = Humidity
X = Visibility
y = Temperature

# Split the dataset into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=4)
print(X_train.shape, X_test.shape, y_train.shape, y_test.shape)
print()


# Initialize and fit the linear regression model using least squares
model = LinearRegressionLS()
model.fit(X_train, y_train)

# Make predictions on the testing set
y_pred = model.predict(X_test)

# Calculate Mean Squared Error
mse = mean_squared_error(y_test, y_pred)
print("Mean Squared Error:", mse)

# Plot the results
plt.figure(figsize=(10, 6))
plt.scatter(X_train, y_train, color='blue', label='Original data')
plt.plot(X_test, y_pred, color='red', label='Fitted line')
plt.xlabel('X')
plt.ylabel('y')
plt.title('Linear Regression with Least Squares')
plt.legend()
plt.grid(True)
plt.show()

# Initialize and fit the RLS model
rls = RecursiveLeastSquares(n_features=1)
errors = rls.fit(X_train, y_train)

# Make predictions
y_pred = rls.predict(X)

# Calculate Mean Squared Error
mse = np.mean(np.array(errors)**2)
print("Mean Squared Error:", mse)

# Plot the results
plt.figure(figsize=(10, 6))
plt.scatter(X, y, color='blue', label='Original data')
plt.plot(X, y_pred, color='red', label='Fitted line')
plt.xlabel('X')
plt.ylabel('y')
plt.title('Recursive Least Squares')
plt.legend()
plt.grid(True)
plt.show()

# Split the dataset into features (X) and target variable (y)
X = Humidity
y = Temperature

# Split the dataset into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Initialize and fit the RLS model
rls = RecursiveLeastSquares(n_features=X_train.shape[1], forgetting_factor=0.99)
errors = rls.fit(X_train, y_train)

# Make predictions
y_pred = rls.predict(X_test)

# Calculate Mean Squared Error
mse = np.mean(np.array(errors)**2)
print("Mean Squared Error:", mse)

# Plot the results
plt.figure(figsize=(10, 6))
plt.scatter(X_train, y_train, color='blue', label='Original data')
plt.plot(X_test, y_pred, color='red', label='Fitted line')
plt.xlabel('X')
plt.ylabel('y')
plt.title('Recursive Least Squares')
plt.legend()
plt.grid(True)
plt.show()

"""## 3-3"""

X = np.concatenate((Humidity, Temperature, Visibility), axis=1)
y = Apparent_Temperature

# Split the dataset into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=4)
print(X_train.shape, X_test.shape, y_train.shape, y_test.shape)
print()

error_variance = 2  # Modify this value based on your estimation

# Calculate weights based on the estimated variance
weights = 1 / error_variance

# Fit the weighted least squares model
X_with_intercept = sm.add_constant(X_train)  # Add intercept term
model = sm.WLS(y_train, X_with_intercept, weights=weights)
result = model.fit()

# Print the model summary
print(result.summary())

X_test_with_intercept = sm.add_constant(X_test)
y_pred = result.predict(X_test_with_intercept)

# Plot Predicted vs. Actual Values
plt.figure(figsize=(8, 6))
plt.scatter(y_pred, y_test, color='blue', alpha=0.5)
plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'k--', lw=2)  # Diagonal line for reference
plt.xlabel('Predicted Values')
plt.ylabel('Actual Values')
plt.title('Predicted vs. Actual Values')
plt.show()