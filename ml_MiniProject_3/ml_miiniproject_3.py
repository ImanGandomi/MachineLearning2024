# -*- coding: utf-8 -*-
"""ML_MIiniProject_3.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1gzlxY9jaKf8ka0gAvrTyIbmCICoAcHqe

#Problem1

## Part a
"""

#import library
import numpy as np
import itertools
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
import matplotlib.pyplot as plt
from sklearn.decomposition import PCA
from sklearn.metrics import accuracy_score
from sklearn.manifold import TSNE
from sklearn.svm import SVC
import pandas as pd
from sklearn import datasets

from sklearn import datasets
iris = datasets.load_iris()

print(iris.keys())

iris['data'].shape

iris['target']

iris['target_names']

iris['feature_names']

data = iris.data
feature_names = iris.feature_names

# Convert to pandas DataFrame
df_data = pd.DataFrame(data, columns=feature_names)
df_data['target'] = iris.target

# Compute statistics
statistics = df_data.describe()

# Print results
print("Summary statistics:")
print(statistics)

df_data.shape

import seaborn as sns

ax = sns.pairplot(df_data, hue='target')
sns.move_legend(
    ax, "lower center",
    bbox_to_anchor=(.5, 1), ncol=3, title="Pair Plot of Wine  Dataset", frameon=False)

plt.tight_layout()
plt.show()

column_names = df_data.columns
column_names = column_names.to_list()

plt.figure(figsize=(18, 9))
for i, feature in enumerate(column_names[:-1]):
    plt.subplot(4, 4, i + 1)
    sns.histplot(data=df_data, x=feature, hue='target', kde=True)
    plt.title(f'{feature} Distribution')
plt.tight_layout()
plt.show()

corr = df_data.corr()
plt.figure(figsize=(10,6))
sns.heatmap(corr,
            xticklabels=corr.columns.values,
            yticklabels=corr.columns.values,
            cmap="BuPu",
            vmin=-1,
            vmax=1,
            annot=True)
plt.title("Correlation Heatmap")
plt.show()

import matplotlib.pyplot as plt

_, ax = plt.subplots()
scatter = ax.scatter(iris.data[:, 0], iris.data[:, 1], c=iris.target)
ax.set(xlabel=iris.feature_names[0], ylabel=iris.feature_names[1])
_ = ax.legend(
    scatter.legend_elements()[0], iris.target_names, loc="lower right", title="Classes"
)

import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
from sklearn import datasets
from sklearn.manifold import TSNE

X = iris.data
y = iris.target

tsne = TSNE(n_components=2, random_state=0)
X_tsne = tsne.fit_transform(X)

plt.figure(figsize=(8, 6))
plt.scatter(X_tsne[:, 0], X_tsne[:, 1], c=y, cmap='viridis')
plt.xlabel('t-SNE Component 1')
plt.ylabel('t-SNE Component 2')
plt.title('t-SNE plot of Iris dataset')
plt.colorbar(label='Species', ticks=range(3), format=plt.FuncFormatter(lambda val, loc: iris.target_names[val]))
plt.show()

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=4)

"""## Part b

"""

pca = PCA(n_components=2)
X_pca = pca.fit_transform(X)

from sklearn.svm import SVC
import matplotlib.pyplot as plt
import numpy as np


clf = SVC(kernel='linear')
clf.fit(X_pca, y)

clf.support_vectors_

clf.n_support_

from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay

# Predict the labels for the test set
y_pred = clf.predict(X_pca)

# Generate and display the confusion matrix
cm = confusion_matrix(y, y_pred)
disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=iris.target_names)
disp.plot()

# Display the plot
plt.show()

from sklearn.svm import SVC
import matplotlib.pyplot as plt
import numpy as np


plt.scatter(X_pca[:, 0], X_pca[:, 1], c=y, cmap='coolwarm', edgecolors='k')

ax = plt.gca()
xlim = ax.get_xlim()
ylim = ax.get_ylim()

xx = np.linspace(xlim[0], xlim[1], 30)
yy = np.linspace(ylim[0], ylim[1], 30)
YY, XX = np.meshgrid(yy, xx)
xy = np.vstack([XX.ravel(), YY.ravel()]).T
Z = clf.decision_function(xy).reshape(XX.shape)

ax.contour(XX, YY, Z, colors='k', levels=[-1, 0, 1], alpha=0.5, linestyles=['--', '-', '--'])
plt.show()

def plot_decision_boundary(X, y, model, title):
    h = .02  # step size in the mesh

    # Create color maps
    cmap_light = plt.cm.viridis
    cmap_bold = plt.cm.Dark2


    y_pred = model.predict(X)
    accuracy = accuracy_score(y, y_pred)

    # Plot the decision boundary. For that, we will assign a color to each point in the mesh [x_min, x_max]x[y_min, y_max].
    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1
    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1
    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),
                         np.arange(y_min, y_max, h))
    Z = model.predict(np.c_[xx.ravel(), yy.ravel()])

    # Put the result into a color plot
    Z = Z.reshape(xx.shape)
    plt.figure()
    plt.contourf(xx, yy, Z, alpha=0.8)
    print(f'Accuracy: {accuracy:.2f}')


    # Plot the training points
    plt.scatter(X[:, 0], X[:, 1], c=y, s=20, edgecolor='k')
    plt.xlim(xx.min(), xx.max())
    plt.ylim(yy.min(), yy.max())
    plt.title(title)
    plt.xlabel('Principal Component 1')
    plt.ylabel('Principal Component 2')
    plt.show()

# Plot decision boundaries with PCA
plot_decision_boundary(X_pca, y, clf, 'Decision Boundaries with PCA')

import numpy as np
import itertools


# Get all combinations of 2 columns out of 4
combinations = list(itertools.combinations(range(X.shape[1]), 2))

# Generate the 6 unique arrays
arrays = [X[:, combination] for combination in combinations]

# Example of accessing individual arrays
# X_12 = arrays[0]
# X_13 = arrays[1]
# X_14 = arrays[2]
# X_23 = arrays[3]
# X_24 = arrays[4]
# X_34 = arrays[5]

combinations

for i in range(6):
  clf = SVC(kernel='linear')
  clf.fit(arrays[i], y)
  plot_decision_boundary(arrays[i], y, clf, f'Decision Boundaries with feature selection: array {i}')
  print("=============================================================================")
  print()

"""## Part c

"""

def plot_decision_boundaries_subplots(X, y):
    degrees = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]
    models = [SVC(kernel='poly', degree=degree, C=1.0) for degree in degrees]

    for model, degree in zip(models, degrees):
      model.fit(X_pca, y)
      y_pred = model.predict(X_pca)
      accuracy = accuracy_score(y, y_pred)

      plot_decision_boundary(X_pca, y, model, f'Decision Boundaries with {degree} degree poly')
      print("===========================================================")
      print()


# Generate synthetic data
plot_decision_boundaries_subplots(X, y)

import numpy as np
import matplotlib.pyplot as plt
import imageio.v2 as imageio  # Importing imageio.v2 to avoid deprecation warning
from sklearn.datasets import load_iris
from sklearn.decomposition import PCA
from sklearn.svm import SVC
from sklearn.metrics import accuracy_score
import os

# Load the Iris dataset and perform PCA
iris = load_iris()
X = iris.data
y = iris.target

pca = PCA(n_components=2)
X_pca = pca.fit_transform(X)

def plot_decision_boundary(X, y, model, title):
    h = .02  # step size in the mesh

    # Create color maps
    cmap_light = plt.cm.viridis
    cmap_bold = plt.cm.Dark2

    y_pred = model.predict(X)
    accuracy = accuracy_score(y, y_pred)

    # Plot the decision boundary. For that, we will assign a color to each point in the mesh [x_min, x_max]x[y_min, y_max].
    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1
    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1
    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),
                         np.arange(y_min, y_max, h))
    Z = model.predict(np.c_[xx.ravel(), yy.ravel()])

    # Put the result into a color plot
    Z = Z.reshape(xx.shape)
    plt.figure()
    plt.contourf(xx, yy, Z, cmap=cmap_light, alpha=0.8)
    print(f'Accuracy: {accuracy:.2f}')

    # Plot the training points
    plt.scatter(X[:, 0], X[:, 1], c=y, cmap=cmap_bold, s=20, edgecolor='k')
    plt.xlim(xx.min(), xx.max())
    plt.ylim(yy.min(), yy.max())
    plt.title(title)
    plt.xlabel('Principal Component 1')
    plt.ylabel('Principal Component 2')
    plt.show()

def plot_decision_boundaries_subplots(X, y):
    degrees = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]
    models = [SVC(kernel='poly', degree=degree, C=1.0) for degree in degrees]
    filenames = []

    for model, degree in zip(models, degrees):
        model.fit(X, y)
        y_pred = model.predict(X)
        accuracy = accuracy_score(y, y_pred)

        # Plot and save each figure
        fig, ax = plt.subplots()
        h = .02  # step size in the mesh
        x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1
        y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1
        xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))
        Z = model.predict(np.c_[xx.ravel(), yy.ravel()])
        Z = Z.reshape(xx.shape)

        ax.contourf(xx, yy, Z, cmap=plt.cm.viridis, alpha=0.8)
        scatter = ax.scatter(X[:, 0], X[:, 1], c=y, cmap=plt.cm.Dark2, edgecolor='k', s=20)
        legend1 = ax.legend(*scatter.legend_elements(), title="Classes")
        ax.add_artist(legend1)
        ax.set_title(f'Decision Boundaries with {degree} degree poly\nAccuracy: {accuracy:.2f}')
        ax.set_xlabel('Principal Component 1')
        ax.set_ylabel('Principal Component 2')

        # Save the figure
        filename = f'plot_degree_{degree}.png'
        fig.savefig(filename)
        filenames.append(filename)
        plt.close(fig)

    # Create a GIF from the saved images with a 5-second delay for each image
    with imageio.get_writer('decision_boundaries.gif', mode='I', duration=5.0) as writer:
        for filename in filenames:
            image = imageio.imread(filename)  # Using imageio.imread from imageio.v2
            writer.append_data(image)

    # Optionally, clean up the image files
    for filename in filenames:
        os.remove(filename)

# Generate synthetic data
plot_decision_boundaries_subplots(X_pca, y)

"""## Part d"""

# Import necessary libraries
from sklearn.datasets import load_iris
import numpy as np
import cvxopt
import matplotlib.pyplot as plt
from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split
import numpy as np
from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split
from sklearn.svm import SVC
from sklearn.preprocessing import LabelEncoder
from sklearn.decomposition import PCA
import matplotlib.pyplot as plt



# Select only two classes (classes 1 and 2) and encode labels
Xtrain1 = iris.data
Ttrain = iris.target
pca = PCA(n_components=2)
Xtrain = pca.fit_transform(Xtrain1)
#Ttrain=1.0*(Ttrain == 1) -1.0* (Ttrain == 2)
# Standardize the features
Xtrain = (Xtrain - Xtrain.mean(axis=0)) / Xtrain.std()

# Shuffle the data
np.random.seed(1234)
indices = np.random.permutation(len(Ttrain))
x = Xtrain[indices]
y1 = Ttrain[indices]

# Split the data into training and test sets
x_train, x_test, y_train, y_test = train_test_split(x, y1, test_size=0.2, random_state=4)



def linear_kernel( x1, x2):
    return np.dot(x1, x2)


def polynomial_kernel( x, y, C=1.0, d=3):
    return (np.dot(x, y) + C) ** d

def gaussian_kernel( x, y, gamma=0.5):
    return np.exp(-gamma*np.linalg.norm(x - y) ** 2)

def sigmoid_kernel( x, y, alpha=1, C=0.01):
    a= alpha * np.dot(x, y) + C
    return np.tanh(a)

def SVM1(X, X_t, y, C, kernel_type, poly_params=(1, 4), RBF_params=0.5, sigmoid_params=(1, 0.01)):
    kernel_and_params=(kernel_type,poly_params, RBF_params, sigmoid_params,C)
    n_samples, n_features = X.shape
    # Compute the Gram matrix
    K = np.zeros((n_samples, n_samples))
    if kernel_type == 'linear':
        for i in range(n_samples):
            for j in range(n_samples):
                K[i, j] = linear_kernel(X[i], X[j])

    elif kernel_type == 'polynomial':
        for i in range(n_samples):
            for j in range(n_samples):
                K[i, j] = polynomial_kernel(X[i], X[j], poly_params[0], poly_params[1])
    else:
        raise ValueError("Invalid kernel type")

    # construct P, q, A, b, G, h matrices for CVXOPT
    P = cvxopt.matrix(np.outer(y, y) * K)
    q = cvxopt.matrix(np.ones(n_samples) * -1)
    A = cvxopt.matrix(y, (1, n_samples))
    b = cvxopt.matrix(0.0)
    G = cvxopt.matrix(np.vstack((np.diag(np.ones(n_samples) * -1), np.identity(n_samples))))
    h = cvxopt.matrix(np.hstack((np.zeros(n_samples), np.ones(n_samples) * C)))
    # solve QP problem
    cvxopt.solvers.options['show_progress'] = False
    solution = cvxopt.solvers.qp(P, q, G, h, A, b)
    # Lagrange multipliers
    a = np.ravel(solution['x'])
    # Support vectors have non zero lagrange multipliers
    sv = a > 1e-5  # some small threshold

        # Support vectors have non zero lagrange multipliers
    ind = np.arange(len(a))[sv]
    a = a[sv]
    sv_x = X[sv]
    sv_y = y[sv]
    numbers_of_sv=len(sv_y)
    # Bias (For linear it is the intercept):
    bias = 0
    for n in range(len(a)):
        # For all support vectors:
        bias += sv_y[n]
        bias -= np.sum(a * sv_y * K[ind[n], sv])
    bias = bias / len(a)

    # Weight vector
    if kernel_type == 'linear':
        w = np.zeros(n_features)
        for n in range(len(a)):
            w += a[n] * sv_y[n] * sv_x[n]
    else:
        w = None

    y_pred=0
    # Create the decision boundary for the plots. Calculates the hypothesis.
    if w is not None:
        y_pred = np.sign(np.dot(X_t, w) + bias)
    else:
        y_predict = np.zeros(len(X_t))
        for i in range(len(X_t)):
            s = 0
            for a1, sv_y1, sv1 in zip(a ,sv_y, sv_x):
                # a : Lagrange multipliers, sv : support vectors.
                # Hypothesis: sign(sum^S a * y * kernel + b)

                if kernel_type == 'linear':
                    s += a1 * sv_y1 * linear_kernel(X_t[i], sv1)
                if kernel_type=='RBF':
                    s += a1 * sv_y1 * gaussian_kernel(X_t[i], sv1, RBF_params)   # Kernel trick.
                if kernel_type == 'polynomial':
                    s += a1 * sv_y1 * polynomial_kernel(X_t[i], sv1, poly_params[0], poly_params[1])
                if kernel_type == 'sigmoid':
                    s=+ a1 * sv_y1 *sigmoid_kernel( X_t[i],  sv1, sigmoid_params[0], sigmoid_params[1])
            y_predict[i] = s
        y_pred = np.sign(y_predict + bias)

    return w, bias, solution,a, sv_x, sv_y, y_pred, kernel_and_params



def multiclass_svm(X,X_t, y, C, kernel_type, poly_params=(1, 4), RBF_params=0.5, sigmoid_params=(1, 0.01)):

    # Step 1: Identify unique class labels
    class_labels = list(set(y))

    # Step 2: Initialize classifiers dictionary
    classifiers = {}
    w_catch={} #catching w, b only for plot part
    b_catch={}
    a_catch={}
    sv_x_catch={}
    sv_y_catch={}
    # Step 3: Train binary SVM models for each required class combination
    for i,class_label in enumerate(class_labels):
        # Create binary labels for current class vs. all others
        binary_y = np.where(y == class_label, 1.0, -1.0)
        # Train SVM classifier for binary classification
        w, bias, _,a, sv_x, sv_y,prediction, kernel_and_params=SVM1(X,X_t, binary_y, C,kernel_type,poly_params, RBF_params, sigmoid_params)
        classifiers[class_label] = prediction
        w_catch[class_label]=w
        b_catch[class_label]=bias
        a_catch[class_label]=a
        sv_x_catch[class_label]=sv_x
        sv_y_catch[class_label]=sv_y
    '''
    a=np.hstack((classifiers[0],classifiers[1],classifiers[2]))
    np.save('array_file', a)
    '''



    def decision_function(X_t):
        decision_scores = np.zeros((X_t.shape[0], len(class_labels)))
        for i, label in enumerate(class_labels):
            decision_scores[:, i] = classifiers[label]
        return np.argmax(decision_scores, axis=1),kernel_and_params,w_catch, b_catch,classifiers
    return decision_function(X_t)

X_train, X_test, y_train, y_test = train_test_split(x, y1, test_size=0.2, random_state=42)

#*****here is the callable function *****--------------------------------------------------------------

model=multiclass_svm(x_train,x_test, y_train, 10, kernel_type ='polynomial', poly_params=(1, 4), RBF_params=0.5, sigmoid_params=(1, 0.01)) #just change linear to polynomial to see what happens!! :)
pred, kernel_and_params,w_catch, b_catch, classifiers=model


#*****here is the callable function *****--------------------------------------------------------------

from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split
from sklearn.svm import SVC
from sklearn.metrics import accuracy_score
from sklearn.preprocessing import LabelEncoder
from sklearn.decomposition import PCA
import matplotlib.pyplot as plt
import numpy as np
from sklearn.decomposition import PCA

def visualize_multiclass_classification(X_train, y_train1, kernel_type, trainset, classifiers, class_labels, w_stack, b_stack,kernel_and_params):
    plt.figure(figsize=(8, 6))
    (_,poly_params, RBF_params, sigmoid_params,C) = kernel_and_params
    # Plotting data points for each class
    for i, target_name in enumerate(class_labels):
        plt.scatter(X_train[y_train1 == i, 0], X_train[y_train1 == i, 1], label=target_name)

    if kernel_type == 'linear':

        h = .02  # step size in the mesh
        x_min, x_max = X_train[:, 0].min() - 1, X_train[:, 0].max() + 1
        y_min, y_max = X_train[:, 1].min() - 1, X_train[:, 1].max() + 1
        k=np.arange(x_min, x_max, h)
        xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))
        x_test = np.c_[xx.ravel(), yy.ravel()]
        model=multiclass_svm(x_train,x_test, y_train, C,kernel_type, poly_params, RBF_params, sigmoid_params)
        pred,_,_,_,_=model

        Z = pred.reshape(xx.shape)
        plt.contour(xx, yy, Z, cmap=plt.cm.Paired, alpha=0.8)

    else:
        h = .02  # step size in the mesh
        x_min, x_max = X_train[:, 0].min() - 1, X_train[:, 0].max() + 1
        y_min, y_max = X_train[:, 1].min() - 1, X_train[:, 1].max() + 1
        k=np.arange(x_min, x_max, h)
        xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))
        x_test = np.c_[xx.ravel(), yy.ravel()]
        model=multiclass_svm(x_train,x_test, y_train, C,kernel_type, poly_params, RBF_params, sigmoid_params)
        pred,_,_,_,_=model

        Z = pred.reshape(xx.shape)
        plt.contour(xx, yy, Z, cmap=plt.cm.Paired, alpha=0.8)


    if trainset:
        plt.title('Data Points on Train Set')
    else:
        plt.title('Data Points on Test Set')

    plt.xlabel('Principal Component 1')
    plt.ylabel('Principal Component 2')
    plt.legend()
    plt.xlim(np.min(X_train[:, 0]) - 1, np.max(X_train[:, 0]) + 1)
    plt.ylim(np.min(X_train[:, 1]) - 1, np.max(X_train[:, 1]) + 1)
    plt.show()

# Now call visualize_multiclass_classification with the appropriate arguments
iris = load_iris()
class_0 = 0
class_1 = 1
class_2 = 2



def visualize_multiclass_classification1(X_train, y_train1, kernel_type, trainset, classifiers, class_labels, w_stack, b_stack):
    plt.figure(figsize=(8, 6))
    # Plotting data points for each class
    for i, target_name in enumerate(class_labels):
        plt.scatter(X_train[y_train1 == i, 0], X_train[y_train1 == i, 1], label=target_name)

    if kernel_type == 'linear':
        for i in range(len(class_labels)):
            w=w_stack[i]
            bias=b_stack[i]
            x_points = np.linspace(np.min(X_train[:, 0]) - 1, np.max(X_train[:, 0]) + 1, 200)
            y_points = -(w[0] / w[1]) * x_points - bias / w[1]
            plt.plot(x_points, y_points, c='r', label='Decision Boundary')

    if trainset:
        plt.title('Data Points on Train Set')
    else:
        plt.title('Data Points on Test Set')

    plt.xlabel('Principal Component 1')
    plt.ylabel('Principal Component 2')
    plt.legend()
    plt.xlim(np.min(X_train[:, 0]) - 1, np.max(X_train[:, 0]) + 1)
    plt.ylim(np.min(X_train[:, 1]) - 1, np.max(X_train[:, 1]) + 1)
    plt.show()

iris = load_iris()
class_0 = 0
class_1 = 1
class_2 = 2
if kernel_and_params[0] == 'linear':
    visualize_multiclass_classification1(X_test, y_test,kernel_and_params[0],False, classifiers,iris.target_names[class_0:class_2+2], w_catch, b_catch)
    visualize_multiclass_classification1(X_train, y_train,kernel_and_params[0],True,classifiers, iris.target_names[class_0:class_2+2], w_catch, b_catch)
visualize_multiclass_classification(X_test, y_test, kernel_and_params[0], False, classifiers, iris.target_names[class_0:class_2+2], w_catch, b_catch, kernel_and_params)
visualize_multiclass_classification(X_train, y_train, kernel_and_params[0], True, classifiers, iris.target_names[class_0:class_2+2], w_catch, b_catch, kernel_and_params)


print(iris.target_names[class_0:class_2+2])


from evaluate import calculate_metrics_and_plot
# Evaluate the model on the training set
if __name__ == "__main__":
    y_true = y_test
    y_pred = pred
    calculate_metrics_and_plot(y_true, y_pred, labels=[0, 1, 2])

X_train, X_test, y_train, y_test = train_test_split(x, y1, test_size=0.2, random_state=42)

#*****here is the callable function *****--------------------------------------------------------------

model=multiclass_svm(x_train,x_test, y_train, 10,'polynomial', poly_params=(1, 4), RBF_params=0.5, sigmoid_params=(1, 0.01)) #just change linear to polynomial to see what happens!! :)
pred, kernel_and_params,w_catch, b_catch, classifiers=model


#*****here is the callable function *****--------------------------------------------------------------

from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split
from sklearn.svm import SVC
from sklearn.metrics import accuracy_score
from sklearn.preprocessing import LabelEncoder
from sklearn.decomposition import PCA
import matplotlib.pyplot as plt
import numpy as np
from sklearn.decomposition import PCA

def visualize_multiclass_classification(X_train, y_train1, kernel_type, trainset, classifiers, class_labels, w_stack, b_stack,kernel_and_params):
    plt.figure(figsize=(8, 6))
    (_,poly_params, RBF_params, sigmoid_params,C) = kernel_and_params
    # Plotting data points for each class
    for i, target_name in enumerate(class_labels):
        plt.scatter(X_train[y_train1 == i, 0], X_train[y_train1 == i, 1], label=target_name)

    if kernel_type == 'linear':

        h = .02  # step size in the mesh
        x_min, x_max = X_train[:, 0].min() - 1, X_train[:, 0].max() + 1
        y_min, y_max = X_train[:, 1].min() - 1, X_train[:, 1].max() + 1
        k=np.arange(x_min, x_max, h)
        xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))
        x_test = np.c_[xx.ravel(), yy.ravel()]
        model=multiclass_svm(x_train,x_test, y_train, C,kernel_type, poly_params, RBF_params, sigmoid_params)
        pred,_,_,_,_=model

        Z = pred.reshape(xx.shape)
        plt.contour(xx, yy, Z, cmap=plt.cm.Paired, alpha=0.8)

    else:
        h = .02  # step size in the mesh
        x_min, x_max = X_train[:, 0].min() - 1, X_train[:, 0].max() + 1
        y_min, y_max = X_train[:, 1].min() - 1, X_train[:, 1].max() + 1
        k=np.arange(x_min, x_max, h)
        xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))
        x_test = np.c_[xx.ravel(), yy.ravel()]
        model=multiclass_svm(x_train,x_test, y_train, C,kernel_type, poly_params, RBF_params, sigmoid_params)
        pred,_,_,_,_=model

        Z = pred.reshape(xx.shape)
        plt.contour(xx, yy, Z, cmap=plt.cm.Paired, alpha=0.8)


    if trainset:
        plt.title('Data Points on Train Set')
    else:
        plt.title('Data Points on Test Set')

    plt.xlabel('Principal Component 1')
    plt.ylabel('Principal Component 2')
    plt.legend()
    plt.xlim(np.min(X_train[:, 0]) - 1, np.max(X_train[:, 0]) + 1)
    plt.ylim(np.min(X_train[:, 1]) - 1, np.max(X_train[:, 1]) + 1)
    plt.show()

# Now call visualize_multiclass_classification with the appropriate arguments
iris = load_iris()
class_0 = 0
class_1 = 1
class_2 = 2



def visualize_multiclass_classification1(X_train, y_train1, kernel_type, trainset, classifiers, class_labels, w_stack, b_stack):
    plt.figure(figsize=(8, 6))
    # Plotting data points for each class
    for i, target_name in enumerate(class_labels):
        plt.scatter(X_train[y_train1 == i, 0], X_train[y_train1 == i, 1], label=target_name)

    if kernel_type == 'linear':
        for i in range(len(class_labels)):
            w=w_stack[i]
            bias=b_stack[i]
            x_points = np.linspace(np.min(X_train[:, 0]) - 1, np.max(X_train[:, 0]) + 1, 200)
            y_points = -(w[0] / w[1]) * x_points - bias / w[1]
            plt.plot(x_points, y_points, c='r', label='Decision Boundary')

    if trainset:
        plt.title('Data Points on Train Set')
    else:
        plt.title('Data Points on Test Set')

    plt.xlabel('Principal Component 1')
    plt.ylabel('Principal Component 2')
    plt.legend()
    plt.xlim(np.min(X_train[:, 0]) - 1, np.max(X_train[:, 0]) + 1)
    plt.ylim(np.min(X_train[:, 1]) - 1, np.max(X_train[:, 1]) + 1)
    plt.show()

iris = load_iris()
class_0 = 0
class_1 = 1
class_2 = 2
if kernel_and_params[0] == 'linear':
    visualize_multiclass_classification1(X_test, y_test,kernel_and_params[0],False, classifiers,iris.target_names[class_0:class_2+2], w_catch, b_catch)
    visualize_multiclass_classification1(X_train, y_train,kernel_and_params[0],True,classifiers, iris.target_names[class_0:class_2+2], w_catch, b_catch)
visualize_multiclass_classification(X_test, y_test, kernel_and_params[0], False, classifiers, iris.target_names[class_0:class_2+2], w_catch, b_catch, kernel_and_params)
visualize_multiclass_classification(X_train, y_train, kernel_and_params[0], True, classifiers, iris.target_names[class_0:class_2+2], w_catch, b_catch, kernel_and_params)


print(iris.target_names[class_0:class_2+2])


from evaluate import calculate_metrics_and_plot
# Evaluate the model on the training set
if __name__ == "__main__":
    y_true = y_test
    y_pred = pred
    calculate_metrics_and_plot(y_true, y_pred, labels=[0, 1, 2])

# Import necessary libraries
from sklearn.datasets import load_iris
import numpy as np
import cvxopt
import matplotlib.pyplot as plt
from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split
import numpy as np
from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split
from sklearn.svm import SVC
from sklearn.preprocessing import LabelEncoder
from sklearn.decomposition import PCA
import matplotlib.pyplot as plt


# Load the Iris dataset
iris = load_iris()

# Select only two classes (classes 1 and 2) and encode labels
Xtrain1 = iris.data
Ttrain = iris.target
pca = PCA(n_components=2)
Xtrain = pca.fit_transform(Xtrain1)
#Ttrain=1.0*(Ttrain == 1) -1.0* (Ttrain == 2)
# Standardize the features
Xtrain = (Xtrain - Xtrain.mean(axis=0)) / Xtrain.std()

# Shuffle the data
np.random.seed(1234)
indices = np.random.permutation(len(Ttrain))
x = Xtrain[indices]
y1 = Ttrain[indices]

# Split the data into training and test sets
x_train, x_test, y_train, y_test = train_test_split(x, y1, test_size=0.2, random_state=42)



def linear_kernel( x1, x2):
    return np.dot(x1, x2)

def polynomial_kernel( x, y, C=1.0, d=3):
    return (np.dot(x, y) + C) ** d

def gaussian_kernel( x, y, gamma=0.5):
    return np.exp(-gamma*np.linalg.norm(x - y) ** 2)

def sigmoid_kernel( x, y, alpha=1, C=0.01):
    a= alpha * np.dot(x, y) + C
    return np.tanh(a)

def SVM1(X, X_t, y, C, kernel_type, poly_params=(1, 4), RBF_params=0.5, sigmoid_params=(1, 0.01)):
    kernel_and_params=(kernel_type,poly_params, RBF_params, sigmoid_params,C)
    n_samples, n_features = X.shape
    # Compute the Gram matrix
    K = np.zeros((n_samples, n_samples))
    if kernel_type == 'linear':
        for i in range(n_samples):
            for j in range(n_samples):
                K[i, j] = linear_kernel(X[i], X[j])
    elif kernel_type == 'polynomial':
        for i in range(n_samples):
            for j in range(n_samples):
                K[i, j] = polynomial_kernel(X[i], X[j], poly_params[0], poly_params[1])
    elif kernel_type == 'RBF':
        for i in range(n_samples):
            for j in range(n_samples):
                K[i, j] = gaussian_kernel(X[i], X[j], RBF_params)
    elif kernel_type == 'sigmoid':
        for i in range(n_samples):
            for j in range(n_samples):
                K[i, j] = sigmoid_kernel(X[i], X[j], sigmoid_params[0], sigmoid_params[1])
    else:
        raise ValueError("Invalid kernel type")

    # construct P, q, A, b, G, h matrices for CVXOPT
    P = cvxopt.matrix(np.outer(y, y) * K)
    q = cvxopt.matrix(np.ones(n_samples) * -1)
    A = cvxopt.matrix(y, (1, n_samples))
    b = cvxopt.matrix(0.0)
    G = cvxopt.matrix(np.vstack((np.diag(np.ones(n_samples) * -1), np.identity(n_samples))))
    h = cvxopt.matrix(np.hstack((np.zeros(n_samples), np.ones(n_samples) * C)))
    # solve QP problem
    cvxopt.solvers.options['show_progress'] = False
    solution = cvxopt.solvers.qp(P, q, G, h, A, b)
    # Lagrange multipliers
    a = np.ravel(solution['x'])
    # Support vectors have non zero lagrange multipliers
    sv = a > 1e-5  # some small threshold

        # Support vectors have non zero lagrange multipliers
    ind = np.arange(len(a))[sv]
    a = a[sv]
    sv_x = X[sv]
    sv_y = y[sv]
    numbers_of_sv=len(sv_y)
    # Bias (For linear it is the intercept):
    bias = 0
    for n in range(len(a)):
        # For all support vectors:
        bias += sv_y[n]
        bias -= np.sum(a * sv_y * K[ind[n], sv])
    bias = bias / len(a)

    # Weight vector
    if kernel_type == 'linear':
        w = np.zeros(n_features)
        for n in range(len(a)):
            w += a[n] * sv_y[n] * sv_x[n]
    else:
        w = None

    y_pred=0
    # Create the decision boundary for the plots. Calculates the hypothesis.
    if w is not None:
        y_pred = np.sign(np.dot(X_t, w) + bias)
    else:
        y_predict = np.zeros(len(X_t))
        for i in range(len(X_t)):
            s = 0
            for a1, sv_y1, sv1 in zip(a ,sv_y, sv_x):
                # a : Lagrange multipliers, sv : support vectors.
                # Hypothesis: sign(sum^S a * y * kernel + b)

                if kernel_type == 'linear':
                    s += a1 * sv_y1 * linear_kernel(X_t[i], sv1)
                if kernel_type=='RBF':
                    s += a1 * sv_y1 * gaussian_kernel(X_t[i], sv1, RBF_params)   # Kernel trick.
                if kernel_type == 'polynomial':
                    s += a1 * sv_y1 * polynomial_kernel(X_t[i], sv1, poly_params[0], poly_params[1])
                if kernel_type == 'sigmoid':
                    s=+ a1 * sv_y1 *sigmoid_kernel( X_t[i],  sv1, sigmoid_params[0], sigmoid_params[1])
            y_predict[i] = s
        y_pred = np.sign(y_predict + bias)

    return w, bias, solution,a, sv_x, sv_y, y_pred, kernel_and_params



def multiclass_svm(X,X_t, y, C, kernel_type, poly_params=(1, 4), RBF_params=0.5, sigmoid_params=(1, 0.01)):

    # Step 1: Identify unique class labels
    class_labels = list(set(y))

    # Step 2: Initialize classifiers dictionary
    classifiers = {}
    w_catch={} #catching w, b only for plot part
    b_catch={}
    a_catch={}
    sv_x_catch={}
    sv_y_catch={}
    # Step 3: Train binary SVM models for each required class combination
    for i,class_label in enumerate(class_labels):
        # Create binary labels for current class vs. all others
        binary_y = np.where(y == class_label, 1.0, -1.0)
        # Train SVM classifier for binary classification
        w, bias, _,a, sv_x, sv_y,prediction, kernel_and_params=SVM1(X,X_t, binary_y, C,kernel_type,poly_params, RBF_params, sigmoid_params)
        classifiers[class_label] = prediction
        w_catch[class_label]=w
        b_catch[class_label]=bias
        a_catch[class_label]=a
        sv_x_catch[class_label]=sv_x
        sv_y_catch[class_label]=sv_y
    '''
    a=np.hstack((classifiers[0],classifiers[1],classifiers[2]))
    np.save('array_file', a)
    '''



    def decision_function(X_t):
        decision_scores = np.zeros((X_t.shape[0], len(class_labels)))
        for i, label in enumerate(class_labels):
            decision_scores[:, i] = classifiers[label]
        return np.argmax(decision_scores, axis=1),kernel_and_params,w_catch, b_catch,classifiers
    return decision_function(X_t)

X_train, X_test, y_train, y_test = train_test_split(x, y1, test_size=0.2, random_state=42)

#*****here is the callable function *****--------------------------------------------------------------

model=multiclass_svm(x_train,x_test, y_train, 10,'polynomial', poly_params=(1, 10), RBF_params=0.5, sigmoid_params=(1, 0.01)) #just change linear to polynomial to see what happens!! :)
pred, kernel_and_params,w_catch, b_catch, classifiers=model


#*****here is the callable function *****--------------------------------------------------------------

from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split
from sklearn.svm import SVC
from sklearn.metrics import accuracy_score
from sklearn.preprocessing import LabelEncoder
from sklearn.decomposition import PCA
import matplotlib.pyplot as plt
import numpy as np
from sklearn.decomposition import PCA

def visualize_multiclass_classification(X_train, y_train1, kernel_type, trainset, classifiers, class_labels, w_stack, b_stack,kernel_and_params):
    plt.figure(figsize=(8, 6))
    (_,poly_params, RBF_params, sigmoid_params,C) = kernel_and_params
    # Plotting data points for each class
    for i, target_name in enumerate(class_labels):
        plt.scatter(X_train[y_train1 == i, 0], X_train[y_train1 == i, 1], label=target_name)

    if kernel_type == 'linear':

        h = .02  # step size in the mesh
        x_min, x_max = X_train[:, 0].min() - 1, X_train[:, 0].max() + 1
        y_min, y_max = X_train[:, 1].min() - 1, X_train[:, 1].max() + 1
        k=np.arange(x_min, x_max, h)
        xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))
        x_test = np.c_[xx.ravel(), yy.ravel()]
        model=multiclass_svm(x_train,x_test, y_train, C,kernel_type, poly_params, RBF_params, sigmoid_params)
        pred,_,_,_,_=model

        Z = pred.reshape(xx.shape)
        plt.contour(xx, yy, Z, cmap=plt.cm.Paired, alpha=0.8)

    else:
        h = .02  # step size in the mesh
        x_min, x_max = X_train[:, 0].min() - 1, X_train[:, 0].max() + 1
        y_min, y_max = X_train[:, 1].min() - 1, X_train[:, 1].max() + 1
        k=np.arange(x_min, x_max, h)
        xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))
        x_test = np.c_[xx.ravel(), yy.ravel()]
        model=multiclass_svm(x_train,x_test, y_train, C,kernel_type, poly_params, RBF_params, sigmoid_params)
        pred,_,_,_,_=model

        Z = pred.reshape(xx.shape)
        plt.contour(xx, yy, Z, cmap=plt.cm.Paired, alpha=0.8)


    if trainset:
        plt.title('Data Points on Train Set')
    else:
        plt.title('Data Points on Test Set')

    plt.xlabel('Principal Component 1')
    plt.ylabel('Principal Component 2')
    plt.legend()
    plt.xlim(np.min(X_train[:, 0]) - 1, np.max(X_train[:, 0]) + 1)
    plt.ylim(np.min(X_train[:, 1]) - 1, np.max(X_train[:, 1]) + 1)
    plt.show()

# Now call visualize_multiclass_classification with the appropriate arguments
iris = load_iris()
class_0 = 0
class_1 = 1
class_2 = 2



def visualize_multiclass_classification1(X_train, y_train1, kernel_type, trainset, classifiers, class_labels, w_stack, b_stack):
    plt.figure(figsize=(8, 6))
    # Plotting data points for each class
    for i, target_name in enumerate(class_labels):
        plt.scatter(X_train[y_train1 == i, 0], X_train[y_train1 == i, 1], label=target_name)

    if kernel_type == 'linear':
        for i in range(len(class_labels)):
            w=w_stack[i]
            bias=b_stack[i]
            x_points = np.linspace(np.min(X_train[:, 0]) - 1, np.max(X_train[:, 0]) + 1, 200)
            y_points = -(w[0] / w[1]) * x_points - bias / w[1]
            plt.plot(x_points, y_points, c='r', label='Decision Boundary')

    if trainset:
        plt.title('Data Points on Train Set')
    else:
        plt.title('Data Points on Test Set')

    plt.xlabel('Principal Component 1')
    plt.ylabel('Principal Component 2')
    plt.legend()
    plt.xlim(np.min(X_train[:, 0]) - 1, np.max(X_train[:, 0]) + 1)
    plt.ylim(np.min(X_train[:, 1]) - 1, np.max(X_train[:, 1]) + 1)
    plt.show()

iris = load_iris()
class_0 = 0
class_1 = 1
class_2 = 2
if kernel_and_params[0] == 'linear':
    visualize_multiclass_classification1(X_test, y_test,kernel_and_params[0],False, classifiers,iris.target_names[class_0:class_2+2], w_catch, b_catch)
    visualize_multiclass_classification1(X_train, y_train,kernel_and_params[0],True,classifiers, iris.target_names[class_0:class_2+2], w_catch, b_catch)
visualize_multiclass_classification(X_test, y_test, kernel_and_params[0], False, classifiers, iris.target_names[class_0:class_2+2], w_catch, b_catch, kernel_and_params)
visualize_multiclass_classification(X_train, y_train, kernel_and_params[0], True, classifiers, iris.target_names[class_0:class_2+2], w_catch, b_catch, kernel_and_params)


print(iris.target_names[class_0:class_2+2])


# from evaluate import calculate_metrics_and_plot
# # Evaluate the model on the training set
# if __name__ == "__main__":
#     y_true = y_test
#     y_pred = pred
#     calculate_metrics_and_plot(y_true, y_pred, labels=[0, 1, 2])

import numpy as np
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, ConfusionMatrixDisplay
import matplotlib.pyplot as plt

def calculate_metrics_and_plot(y_true, y_pred, labels):
    # Calculate metrics
    accuracy = accuracy_score(y_true, y_pred)
    precision = precision_score(y_true, y_pred, average='weighted', labels=labels)
    recall = recall_score(y_true, y_pred, average='weighted', labels=labels)
    f1 = f1_score(y_true, y_pred, average='weighted', labels=labels)

    # Print metrics
    print(f"Accuracy: {accuracy:.2f}")
    print(f"Precision: {precision:.2f}")
    print(f"Recall: {recall:.2f}")
    print(f"F1 Score: {f1:.2f}")

    # Confusion Matrix
    cm = confusion_matrix(y_true, y_pred, labels=labels)
    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=labels)
    disp.plot(cmap=plt.cm.Blues)
    plt.title('Confusion Matrix')
    plt.show()

# Example usage
if __name__ == "__main__":
    # Dummy true and predicted labels
    y_true = y_test
    y_pred = pred
    labels = [0, 1, 2]

    calculate_metrics_and_plot(y_true, y_pred, labels)

!pip install imageio

!pip install pillow imageio

import imageio
from PIL import Image
import os
import numpy as np

# Directory where images are stored
image_directory = '/content/Untitled Folder'

# List files in the directory
files_in_directory = os.listdir(image_directory)
print("Files in directory:", files_in_directory)

# Get list of all image files in the directory
image_files = [os.path.join(image_directory, file) for file in files_in_directory if file.lower().endswith(('png', 'jpg', 'jpeg'))]

# Debug: Print the list of image files
print("Image files found:", image_files)

# Check if there are any images
if not image_files:
    raise ValueError("No images found in the directory.")

# Sort the images by name (optional, depends on how you want to order them in the GIF)
image_files.sort()

# Read images, resize them, and store in a list
images = []
target_size = (500, 500)  # You can change this to the desired size

for file in image_files:
    try:
        img = Image.open(file)
        img_resized = img.resize(target_size, Image.ANTIALIAS)
        images.append(np.array(img_resized))  # Convert PIL.Image.Image to numpy array
    except Exception as e:
        print(f"Error reading or processing {file}: {e}")

# Debug: Check if images list is populated
if not images:
    raise ValueError("No images were successfully read.")

# Create and save GIF
gif_path = '/content/output.gif'
imageio.mimsave(gif_path, images, duration=0.5)  # duration is the time between frames in seconds

print(f"GIF saved at {gif_path}")

"""# Problem 3

##Part a
"""

# در گزارش کار بصورت توضیحی آورده شده است

"""## Part b"""

# در گزارش کار بصورت توضیحی آورده شده است

"""## Part c"""

!gdown 1kZz6EEvgGF_rmgWe1KCVIbAehjp029O8

# Set device
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, TensorDataset, random_split
from sklearn.preprocessing import StandardScaler
from imblearn.over_sampling import SMOTE
from sklearn.model_selection import train_test_split
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score
import seaborn as sns

file_path = "/content/creditcard.csv"
dataset = pd.read_csv(file_path)
dataset.dropna()

dataset.shape

dataset.describe()

print("Few Entries: ")
print(dataset.head())
print("Dataset Shape: ", dataset.shape)
print("Maximum Transaction Value: ", np.max(dataset.Amount))
print("Minimum Transaction Value: ", np.min(dataset.Amount))

feature = dataset.drop(["Time", "Class"], axis=1)
feature.shape

target = dataset['Class']
target.shape

X = feature
y = target

# Normalize the 'Amount' column
scaler = StandardScaler()
X['Amount'] = scaler.fit_transform(X[['Amount']])

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=4)

# Denoising Autoencoder
input_dim = X.shape[1]
encoding_dim = 10  # The bottleneck layer

input_layer = Input(shape=(input_dim,))
encoded = Dense(22, activation='relu')(input_layer)
encoded = Dense(15, activation='relu')(encoded)
encoded = Dense(encoding_dim, activation='relu')(encoded)

decoded = Dense(15, activation='relu')(encoded)
decoded = Dense(22, activation='relu')(decoded)
decoded = Dense(input_dim, activation='sigmoid')(decoded)

autoencoder = Model(input_layer, decoded)
autoencoder.compile(optimizer='adam', loss='mean_squared_error')

# Train the autoencoder
autoencoder.fit(X_noisy, X_resampled, epochs=50, batch_size=256, shuffle=True, validation_split=0.2)

# Dense Autoencoder model
class Dense_Autoencoder(nn.Module):
    def __init__(self):
        super().__init__()
        input_dim = X.shape[1]
        encoding_dim = 10  # The bottleneck layer
        self.encoder = nn.Sequential(
            nn.Linear(28 * 28, 128),
            nn.ReLU(),
            nn.Linear(128, 64),
            nn.ReLU(),
            nn.Linear(64, 12),
            nn.ReLU(),
            nn.Linear(12, 3)
        )

        self.decoder = nn.Sequential(
            nn.Linear(3, 12),
            nn.ReLU(),
            nn.Linear(12, 64),
            nn.ReLU(),
            nn.Linear(64, 128),
            nn.ReLU(),
            nn.Linear(128, 28 * 28),
            nn.Sigmoid()
        )

    def forward(self, x):
        encoded = self.encoder(x)
        decoded = self.decoder(encoded)
        return encoded, decoded

# Dense Autoencoder model
class Dense_Autoencoder(nn.Module):
    def __init__(self):
        super().__init__()
        self.encoder = nn.Sequential(
            nn.Linear(28 * 28, 128),
            nn.ReLU(),
            nn.Linear(128, 64),
            nn.ReLU(),
            nn.Linear(64, 12),
            nn.ReLU(),
            nn.Linear(12, 3)
        )

        self.decoder = nn.Sequential(
            nn.Linear(3, 12),
            nn.ReLU(),
            nn.Linear(12, 64),
            nn.ReLU(),
            nn.Linear(64, 128),
            nn.ReLU(),
            nn.Linear(128, 28 * 28),
            nn.Sigmoid()
        )

    def forward(self, x):
        encoded = self.encoder(x)
        decoded = self.decoder(encoded)
        return encoded, decoded









!git clone https://github.com/ImanGandomi/Autoencoder-and-Oversampling-for-credit-card-fraud-detection-on-highly-imbalanced-dataset.git

# Commented out IPython magic to ensure Python compatibility.
# %cd /content/Autoencoder-and-Oversampling-for-credit-card-fraud-detection-on-highly-imbalanced-dataset

!gdown 1kZz6EEvgGF_rmgWe1KCVIbAehjp029O8

!python TrainEvaluateModel.py

"""## Prat c"""

import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, TensorDataset, random_split
from sklearn.preprocessing import StandardScaler
from imblearn.over_sampling import SMOTE
from sklearn.model_selection import train_test_split
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score
import seaborn as sns

!gdown 1kZz6EEvgGF_rmgWe1KCVIbAehjp029O8

# Set device
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

file_path = "/content/creditcard.csv"
data = pd.read_csv(file_path)

X = data.drop(columns=["Time", "Class"]).values
y = data['Class'].values

scaler = StandardScaler()
X[:, -1] = scaler.fit_transform(X[:, -1].reshape(-1, 1)).flatten()

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=4)

# Apply SMOTE to balance the training dataset
smote = SMOTE(sampling_strategy='auto', random_state=4)
X_resampled, y_resampled = smote.fit_resample(X_train, y_train)

# Add Gaussian noise to the training data
noise_factor = 0.5
X_noisy = X_resampled + noise_factor * np.random.normal(loc=0.0, scale=1.0, size=X_resampled.shape)

# Convert to PyTorch tensors
X_noisy_tensor = torch.tensor(X_noisy, dtype=torch.float32).to(device)
X_resampled_tensor = torch.tensor(X_resampled, dtype=torch.float32).to(device)
X_test_tensor = torch.tensor(X_test, dtype=torch.float32).to(device)
y_resampled_tensor = torch.tensor(y_resampled, dtype=torch.long).to(device)
y_test_tensor = torch.tensor(y_test, dtype=torch.long).to(device)

# Create DataLoader
batch_size = 512
train_dataset = TensorDataset(X_noisy_tensor, X_resampled_tensor)
train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)

test_dataset = TensorDataset(X_test_tensor, y_test_tensor)
test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)

class DenoisingAutoencoder(nn.Module):
    def __init__(self):
        super(DenoisingAutoencoder, self).__init__()
        self.encoder = nn.Sequential(
            nn.Linear(29, 22),
            nn.LeakyReLU(),
            nn.Linear(22, 15),
            nn.LeakyReLU(),
            nn.Linear(15, 10),
            nn.LeakyReLU()
        )
        self.decoder = nn.Sequential(
            nn.Linear(10, 15),
            nn.LeakyReLU(),
            nn.Linear(15, 22),
            nn.LeakyReLU(),
            nn.Linear(22, 29),
        )

    def forward(self, x):
        encoded = self.encoder(x)
        decoded = self.decoder(encoded)
        return decoded

# Define Classifier
class Classifier(nn.Module):
    def __init__(self):
        super(Classifier, self).__init__()
        self.classifier = nn.Sequential(
            nn.Linear(29, 22),
            nn.LeakyReLU(),
            nn.Linear(22, 15),
            nn.LeakyReLU(),
            nn.Linear(15, 10),
            nn.LeakyReLU(),
            nn.Linear(10, 5),
            nn.LeakyReLU(),
            nn.Linear(5, 2),
            nn.Softmax(dim=1)
        )

    def forward(self, x):
        return self.classifier(x)

# Instantiate models
autoencoder = DenoisingAutoencoder().to(device)
classifier = Classifier().to(device)

# Define loss function and optimizer
criterion = nn.MSELoss()
optimizer = optim.Adam(autoencoder.parameters(), lr=0.01)

autoencoder_loss = []


# Train Autoencoder
num_epochs1 = 30
for epoch in range(num_epochs1):
    autoencoder.train()
    for batch in train_loader:
        X_noisy_batch, X_resampled_batch = batch
        optimizer.zero_grad()
        outputs = autoencoder(X_noisy_batch)
        loss = criterion(outputs, X_resampled_batch)
        loss.backward()
        optimizer.step()
    print(f'Epoch [{epoch+1}/{num_epochs1}], Loss: {loss.item():.4f}')

# Extract denoised features using the trained autoencoder
autoencoder.eval()
with torch.no_grad():
    X_train_denoised = autoencoder(X_resampled_tensor).cpu().numpy()
    X_test_denoised = autoencoder(X_test_tensor).cpu().numpy()

# Convert denoised features to tensors
X_train_denoised_tensor = torch.tensor(X_train_denoised, dtype=torch.float32).to(device)
X_test_denoised_tensor = torch.tensor(X_test_denoised, dtype=torch.float32).to(device)

# Create DataLoader for denoised data
denoised_train_dataset = TensorDataset(X_train_denoised_tensor, y_resampled_tensor)
denoised_train_loader = DataLoader(denoised_train_dataset, batch_size=batch_size, shuffle=True)

# Define loss function and optimizer for the classifier
criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(classifier.parameters(), lr=0.01)

# Track best loss for saving model
best_loss = float('inf')
best_model_path = 'best_classifier_model.pth'

# For plotting accuracy and recall
train_accuracies = []
train_recalls = []
test_accuracies = []
test_recalls = []

outputs

y_batch

outputs[0].dtype

num_epochs = 60

# Train Classifier
for epoch in range(num_epochs):
    classifier.train()
    running_loss = 0.0
    correct = 0
    total = 0
    y_true = []
    y_pred = []

    for batch in denoised_train_loader:
        X_denoised_batch, y_batch = batch
        optimizer.zero_grad()
        outputs = classifier(X_denoised_batch)
        loss = criterion(outputs, y_batch)
        loss.backward()
        optimizer.step()

        running_loss += loss.item()
        _, predicted = torch.max(outputs.data, 1)
        total += y_batch.size(0)
        correct += (predicted == y_batch).sum().item()
        y_true.extend(y_batch.cpu().numpy())
        y_pred.extend(predicted.cpu().numpy())

    train_accuracy = 100 * correct / total
    train_recall = recall_score(y_true, y_pred)
    train_accuracies.append(train_accuracy)
    train_recalls.append(train_recall)
    avg_loss = running_loss / len(denoised_train_loader)

    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {avg_loss:.4f}, Accuracy: {train_accuracy:.2f}%, Recall: {train_recall:.2f}')

    # Save the best model
    if avg_loss < best_loss:
        best_loss = avg_loss
        torch.save(classifier.state_dict(), best_model_path)

    # Evaluate on test data
    classifier.eval()
    correct = 0
    total = 0
    y_true = []
    y_pred = []

    with torch.no_grad():
        for batch in test_loader:
            X_batch, y_batch = batch
            outputs = classifier(autoencoder(X_batch))
            _, predicted = torch.max(outputs.data, 1)
            total += y_batch.size(0)
            correct += (predicted == y_batch).sum().item()
            y_true.extend(y_batch.cpu().numpy())
            y_pred.extend(predicted.cpu().numpy())

    test_accuracy = 100 * correct / total
    test_recall = recall_score(y_true, y_pred)
    test_accuracies.append(test_accuracy)
    test_recalls.append(test_recall)
    print(f'Test Accuracy: {test_accuracy:.2f}%, Test Recall: {test_recall:.2f}')

num_epochs = 60

# Train Classifier
for epoch in range(num_epochs):
    classifier.train()
    running_loss = 0.0
    correct = 0
    total = 0
    y_true = []
    y_pred = []

    for batch in denoised_train_loader:
        X_denoised_batch, y_batch = batch
        optimizer.zero_grad()
        outputs = classifier(X_denoised_batch)
        loss = criterion(outputs, y_batch)
        loss.backward()
        optimizer.step()

        running_loss += loss.item()
        _, predicted = torch.max(outputs.data, 1)
        total += y_batch.size(0)
        correct += (predicted == y_batch).sum().item()
        y_true.extend(y_batch.cpu().numpy())
        y_pred.extend(predicted.cpu().numpy())

    train_accuracy = 100 * correct / total
    train_recall = recall_score(y_true, y_pred)
    train_accuracies.append(train_accuracy)
    train_recalls.append(train_recall)
    avg_loss = running_loss / len(denoised_train_loader)

    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {avg_loss:.4f}, Accuracy: {train_accuracy:.2f}%, Recall: {train_recall:.2f}')

    # Save the best model
    if avg_loss < best_loss:
        best_loss = avg_loss
        torch.save(classifier.state_dict(), best_model_path)

    # Evaluate on test data
    classifier.eval()
    correct = 0
    total = 0
    y_true = []
    y_pred = []

    with torch.no_grad():
        for batch in test_loader:
            X_batch, y_batch = batch
            outputs = classifier(autoencoder.encoder(X_batch))
            _, predicted = torch.max(outputs.data, 1)
            total += y_batch.size(0)
            correct += (predicted == y_batch).sum().item()
            y_true.extend(y_batch.cpu().numpy())
            y_pred.extend(predicted.cpu().numpy())

    test_accuracy = 100 * correct / total
    test_recall = recall_score(y_true, y_pred)
    test_accuracies.append(test_accuracy)
    test_recalls.append(test_recall)
    print(f'Test Accuracy: {test_accuracy:.2f}%, Test Recall: {test_recall:.2f}')

# Plot Accuracy and Recall
plt.figure(figsize=(12, 6))
plt.subplot(1, 2, 1)
plt.plot(train_accuracies, label='Train Accuracy')
plt.plot(test_accuracies, label='Test Accuracy')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.legend()
plt.title('Accuracy over Epochs')

plt.subplot(1, 2, 2)
plt.plot(train_recalls, label='Train Recall')
plt.plot(test_recalls, label='Test Recall')
plt.xlabel('Epoch')
plt.ylabel('Recall')
plt.legend()
plt.title('Recall over Epochs')

plt.tight_layout()
plt.show()

"""## Part d"""

# Calculate metrics
accuracy = accuracy_score(y_true, y_pred)
precision = precision_score(y_true, y_pred)
recall = recall_score(y_true, y_pred)
f1 = f1_score(y_true, y_pred)

print(f'Accuracy: {accuracy:.4f}')
print(f'Precision: {precision:.4f}')
print(f'Recall: {recall:.4f}')
print(f'F1 Score: {f1:.4f}')

# Plot confusion matrix
cm = confusion_matrix(y_true, y_pred)
plt.figure(figsize=(8, 6))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.title('Confusion Matrix')
plt.show()

from sklearn.metrics import classification_report, confusion_matrix

# Calculate and print the classification report
report = classification_report(y_true, y_pred)
print(report)

"""##Part e"""

def calculate_metrics(outputs , labels, thresholds):
    recalls = []
    accuracies = []
    for threshold in thresholds:
        predictions = (outputs > threshold).astype(int)
        recall = recall_score(labels, predictions)
        accuracy = accuracy_score(labels, predictions)
        recalls.append(recall)
        accuracies.append(accuracy)

    return recalls , accuracies

thresholds = np.linspace(0, 1, 10)

test_recalls, test_accuracies = calculate_metrics(y_pred, y_true, thresholds)

# Plotting the recall and accuracy for different thresholds on the same figure
plt.figure(figsize=(10, 6))

# Plot Recall
plt.plot(thresholds, test_recalls, label='Recall', color='blue')

# Plot Accuracy
plt.plot(thresholds, test_accuracies, label='Accuracy', color='green')

plt.xlabel('Threshold')
plt.ylabel('Metric Value')
plt.title('Recall and Accuracy vs. Threshold')
plt.legend()
plt.grid(True)

plt.show()

"""## Part f"""

# Define Classifier
class Classifier(nn.Module):
    def __init__(self):
        super(Classifier, self).__init__()
        self.classifier = nn.Sequential(
            nn.Linear(29, 22),
            nn.ReLU(),
            nn.Linear(22, 15),
            nn.ReLU(),
            nn.Linear(15, 10),
            nn.ReLU(),
            nn.Linear(10, 5),
            nn.ReLU(),
            nn.Linear(5, 2),
            nn.Softmax(dim=1)
        )

    def forward(self, x):
        return self.classifier(x)

# Instantiate models
classifier = Classifier().to(device)

X = data.drop(columns=["Time", "Class"]).values
y = data['Class'].values

scaler = StandardScaler()
X[:, -1] = scaler.fit_transform(X[:, -1].reshape(-1, 1)).flatten()

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=4)

# Convert denoised features to tensors
X_train_denoised_tensor = torch.tensor(X_train, dtype=torch.float32).to(device)
X_test_denoised_tensor = torch.tensor(X_test, dtype=torch.float32).to(device)

y_resampled_tensor = torch.tensor(y_train, dtype=torch.float32).to(device)

X_train_denoised_tensor.dtype

# Create DataLoader for denoised data
denoised_train_dataset = TensorDataset(X_train_denoised_tensor, y_resampled_tensor)
denoised_train_loader = DataLoader(denoised_train_dataset, batch_size=batch_size, shuffle=True)

# Define loss function and optimizer for the classifier
criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(classifier.parameters(), lr=0.01)



num_epochs = 60

# Train Classifier
for epoch in range(num_epochs):
    classifier.train()
    running_loss = 0.0
    correct = 0
    total = 0
    y_true = []
    y_pred = []

    for batch in denoised_train_loader:
        X_denoised_batch, y_batch = batch
        optimizer.zero_grad()
        outputs = classifier(X_denoised_batch)
        loss = criterion(outputs, y_batch)
        loss.backward()
        optimizer.step()

        running_loss += loss.item()
        _, predicted = torch.max(outputs.data, 1)
        total += y_batch.size(0)
        correct += (predicted == y_batch).sum().item()
        y_true.extend(y_batch.cpu().numpy())
        y_pred.extend(predicted.cpu().numpy())

    train_accuracy = 100 * correct / total
    train_recall = recall_score(y_true, y_pred)
    train_accuracies.append(train_accuracy)
    train_recalls.append(train_recall)
    avg_loss = running_loss / len(denoised_train_loader)

    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {avg_loss:.4f}, Accuracy: {train_accuracy:.2f}%, Recall: {train_recall:.2f}')

    # Save the best model
    if avg_loss < best_loss:
        best_loss = avg_loss
        torch.save(classifier.state_dict(), best_model_path)

    # Evaluate on test data
    classifier.eval()
    correct = 0
    total = 0
    y_true = []
    y_pred = []

    with torch.no_grad():
        for batch in test_loader:
            X_batch, y_batch = batch
            outputs = classifier(autoencoder.encoder(X_batch))
            _, predicted = torch.max(outputs.data, 1)
            total += y_batch.size(0)
            correct += (predicted == y_batch).sum().item()
            y_true.extend(y_batch.cpu().numpy())
            y_pred.extend(predicted.cpu().numpy())

    test_accuracy = 100 * correct / total
    test_recall = recall_score(y_true, y_pred)
    test_accuracies.append(test_accuracy)
    test_recalls.append(test_recall)
    print(f'Test Accuracy: {test_accuracy:.2f}%, Test Recall: {test_recall:.2f}')

