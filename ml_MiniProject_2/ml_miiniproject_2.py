# -*- coding: utf-8 -*-
"""ML_MIiniProject_2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1AjG5WVYKpsJYs8rkoI94V5ZuJdnVG3_m

#Problem1
"""

#import library
import numpy as np
import itertools
import matplotlib.pyplot as plt

#define muculloch pitts
class McCulloch_Pitts_neuron():
  def __init__(self , weights , threshold):
    self.weights = weights    #define weights
    self.threshold = threshold    #define threshold

  def model(self , x):
    #define model with threshold
    y = self.weights @ x
    if y >= self.threshold:
        return 1
    else:
        return 0

#define model for dataset
def Area(x, y):
  neur1 = McCulloch_Pitts_neuron([0, 1], 0)
  neur2 = McCulloch_Pitts_neuron([2, -1], 2)
  neur3 = McCulloch_Pitts_neuron([-2, -1], -6)
  neur4 = McCulloch_Pitts_neuron([1, 1, 1], 3)

  z1 = neur1.model(np.array([x, y]))
  z2 = neur2.model(np.array([x, y]))
  z3 = neur3.model(np.array([x, y]))
  z4 = neur4.model(np.array([z1, z2, z3]))

  # return str(z1) + str(z2)
  return list([z4])

# Plotting
plt.figure(figsize=(8, 6))
plt.xlabel('X values')
plt.ylabel('Y values')
plt.title('McCulloch-Pitts Neuron Outputs')

y_points = [0, 2, 0]
x_points = [1, 2, 3]

plt.plot(x_points[0:2], y_points[0:2], "black")
plt.plot(x_points[1:3], y_points[1:3], "black")
plt.plot([x_points[0], x_points[-1]], [y_points[0], y_points[-1]], "black", label='Triangle')

plt.fill(x_points, y_points, hatch='/', edgecolor='pink', alpha=0.5, label='Area In T')  # Add hatch inside the triangle
plt.grid(True)

# Set axis limits
plt.xlim(0, 4)
plt.ylim(-1, 3)

# Position the legends at the top and right
plt.legend(loc='upper right', bbox_to_anchor=(1.2, 1.0))

# Save plot as PDF
plt.savefig('c.png', bbox_inches='tight')

plt.show()

# Generate random data points
num_points = 2000
x_values = np.random.uniform(0, 4, num_points)
y_values = np.random.uniform(-1, 3, num_points)

# Initialize lists to store data points for different z5 values
red_points = []
green_points = []

# Evaluate data points using the Area function
for i in range(num_points):
    z_value = Area(x_values[i], y_values[i])
    if z_value == [0]:  # z5 value is 0
        red_points.append((x_values[i], y_values[i]))
    else:  # z5 value is 1
        green_points.append((x_values[i], y_values[i]))

# Separate x and y values for red and green points
red_x, red_y = zip(*red_points)
green_x, green_y = zip(*green_points)

# Plotting
plt.figure(figsize=(8, 6))
plt.scatter(red_x, red_y, color='red', label='z = 0')
plt.scatter(green_x, green_y, color='green', label='z = 1')
plt.xlabel('X values')
plt.ylabel('Y values')
plt.title('McCulloch-Pitts Neuron Outputs')


y_points = [0, 2, 0]
x_points = [1, 2, 3]

plt.plot(x_points[0:2], y_points[0:2], "black")
plt.plot(x_points[1:3], y_points[1:3], "black")
plt.plot([x_points[0], x_points[-1]], [y_points[0], y_points[-1]], "black", label='Triangle')
plt.fill(x_points, y_points, hatch='/', edgecolor='pink', alpha=0.5, label='Area In T')  # Add hatch inside the triangle

plt.grid(True)

# Set axis limits
plt.xlim(0, 4)
plt.ylim(-1, 3)

# Position the legends at the top and right
plt.legend(loc='upper right', bbox_to_anchor=(1.2, 1.0))

# Save plot as PDF
plt.savefig('c.png', bbox_inches='tight')

plt.show()

"""## with AF"""

def leaky_relu(x, alpha=0.01):
    return max(alpha*x, x)

def sigmoid(x):
    return 1 / (1 + np.exp(-x))

def linear(x):
  return x

def ReLU(x):
    if x>=0:
      return x
    else:
      return 0

#define muculloch pitts
class McCulloch_Pitts_neuron_with_AF():
  def __init__(self , weights , threshold, activation_func = None):
    self.weights = weights    #define weights
    self.threshold = threshold    #define threshold
    self.activation_func = activation_func

  def model(self , x):
    #define model with threshold
    y = self.weights @ x
    y = y if self.activation_func == None else self.activation_func(y)
    if y >= self.threshold:
        return 1
    else:
        return 0

x_points[0: 2]

#define model for dataset
def Area_with_AF(x, y):
  neur1 = McCulloch_Pitts_neuron_with_AF([0, 1], sigmoid(0), sigmoid)
  neur2 = McCulloch_Pitts_neuron_with_AF([2, -1], sigmoid(2), sigmoid)
  neur3 = McCulloch_Pitts_neuron_with_AF([-2, -1], sigmoid(-6), sigmoid)
  neur4 = McCulloch_Pitts_neuron_with_AF([1, 1, 1], sigmoid(3), sigmoid)

  z1 = neur1.model(np.array([x, y]))
  z2 = neur2.model(np.array([x, y]))
  z3 = neur3.model(np.array([x, y]))
  z4 = neur4.model(np.array([z1, z2, z3]))

  # return str(z1) + str(z2)
  return list([z4])

# Initialize lists to store data points for different z5 values
red_points = []
green_points = []

# Evaluate data points using the Area function
for i in range(num_points):
    z_value = Area_with_AF(x_values[i], y_values[i])
    if z_value == [0]:  # z5 value is 0
        red_points.append((x_values[i], y_values[i]))
    else:  # z5 value is 1
        green_points.append((x_values[i], y_values[i]))

# Separate x and y values for red and green points
red_x, red_y = zip(*red_points)
green_x, green_y = zip(*green_points)

# Plotting
plt.figure(figsize=(8, 6))
plt.scatter(red_x, red_y, color='red', label='z = 0')
plt.scatter(green_x, green_y, color='green', label='z = 1')
plt.xlabel('X values')
plt.ylabel('Y values')
plt.title('McCulloch-Pitts Neuron Outputs')

y_points = [0, 2, 0]
x_points = [1, 2, 3]

plt.plot(x_points[0:2], y_points[0:2], "black")
plt.plot(x_points[1:3], y_points[1:3], "black")
plt.plot([x_points[0], x_points[-1]], [y_points[0], y_points[-1]], "black", label='Triangle')

plt.fill(x_points, y_points, hatch='/', edgecolor='pink', alpha=0.5, label='Area In T')  # Add hatch inside the triangle


plt.grid(True)

# Set axis limits
plt.xlim(0, 4)
plt.ylim(-1, 3)

# Position the legends at the top and right
plt.legend(loc='upper right', bbox_to_anchor=(1.2, 1.0))

# Save plot as PDF
plt.savefig('c.png', bbox_inches='tight')

plt.show()

"""# Problem 2

## Part a
"""

# Commented out IPython magic to ensure Python compatibility.
# %cd /content

!gdown 1VoGTdaylo4ytvh6wMOVTxelgxWlQfvyu
!gdown 10IycnfvXVGsMVRQ58-Qy-l4YpvOfYY-_
!gdown 1u45bE0rKvKVNDkqPtmMEQLBTd1wfo6pc
!gdown 1EE1bGnHm5T10pgmgoI99FEQbVyBbY5Ky

from scipy.io import loadmat
import numpy as np
from scipy.stats import skew, kurtosis
from sklearn.utils import shuffle
from sklearn.model_selection import train_test_split

# Load the .mat file
normal_data = loadmat('/content/97.mat')
fault_data_IR007 = loadmat('/content/105.mat')
fault_data_BB007 = loadmat('/content/118.mat')
fault_data_OR007 = loadmat('/content/130.mat')

print(normal_data.keys())
print(fault_data_IR007.keys())
print(fault_data_BB007.keys())
print(fault_data_OR007.keys())

normal_data_variable = normal_data['X097_DE_time']
print(type(normal_data_variable))
print(normal_data_variable.shape)

fault_data_IR007_variable = fault_data_IR007['X105_DE_time']
print(fault_data_IR007_variable.shape)

fault_data_BB007_variable = fault_data_BB007['X118_DE_time']
print(fault_data_BB007_variable.shape)

fault_data_OR007_variable = fault_data_OR007['X130_DE_time']
print(fault_data_OR007_variable.shape)

# Extract 10 samples, each containing 5 data points
n_samples = 120
n_data = 220
ext_normal_data = normal_data_variable[:n_samples * n_data, 0]
ext_fault_data_IR007 = fault_data_IR007_variable[:n_samples * n_data, 0]
ext_fault_data_BB007 = fault_data_BB007_variable[:n_samples * n_data, 0]
ext_fault_data_OR007 = fault_data_OR007_variable[:n_samples * n_data, 0]


print(ext_normal_data.shape)
print(ext_fault_data_IR007.shape)
print(ext_fault_data_BB007.shape)
print(ext_fault_data_OR007.shape)

# Reshape the extracted data to have 10 rows and 5 columns
ext_normal_data = ext_normal_data.reshape(n_samples, n_data)
ext_fault_data_IR007 = ext_fault_data_IR007.reshape(n_samples, n_data)
ext_fault_data_BB007 = ext_fault_data_BB007.reshape(n_samples, n_data)
ext_fault_data_OR007 = ext_fault_data_OR007.reshape(n_samples, n_data)


print(ext_normal_data.shape)
print(ext_fault_data_IR007.shape)
print(ext_fault_data_BB007.shape)
print(ext_fault_data_OR007.shape)

ext_normal_data[:, 0]

def feature_extraction (data, class_num):
  #Normal data feature extraction
  standard_deviations = np.std(data, axis=1)
  skewnesses = skew(data, axis=1)
  kurtoses = kurtosis(data, axis=1)
  peak_to_peaks = np.ptp(data, axis=1)
  root_mean_squares = np.sqrt(np.mean(np.square(data), axis=1))
  means = np.mean(data, axis=1)
  absolute_means = np.mean(np.abs(data), axis=1)
  peaks = np.max(data, axis=1)

  print("Standard Deviations:", standard_deviations.shape)
  print("Skewnesses:", skewnesses.shape)
  print("Kurtoses:", kurtoses.shape)
  print("Peak to Peaks:", peak_to_peaks.shape)
  print("Root Mean Squares:", root_mean_squares.shape)
  print("Means:", means.shape)
  print("Absolute Means:", absolute_means.shape)
  print("Peaks:", peaks.shape)

  ext_feature_dataset = np.column_stack((standard_deviations, skewnesses, kurtoses, peak_to_peaks,
                                root_mean_squares, means, absolute_means, peaks))


  ext_feature_dataset = np.hstack((ext_feature_dataset, class_num * np.ones((len(ext_feature_dataset), 1))))
  print("final dataset :",ext_feature_dataset.shape)


  return ext_feature_dataset

normal_ext_feature_dataset = feature_extraction(ext_normal_data, 0)

IR007_ext_feature_dataset = feature_extraction(ext_fault_data_IR007, 1)

BB007_ext_feature_dataset = feature_extraction(ext_fault_data_BB007, 2)

OR007_ext_feature_dataset = feature_extraction(ext_fault_data_OR007, 3)

final_data = np.concatenate((normal_ext_feature_dataset, IR007_ext_feature_dataset, BB007_ext_feature_dataset, OR007_ext_feature_dataset), axis=0)
final_data.shape

shuffled_final_data = shuffle(final_data, random_state=4)
shuffled_final_data.shape

X = shuffled_final_data[:, :-1]
y = shuffled_final_data[:, -1]
print("X :",X.shape)
print("y :",y.shape)

# x_train_plus_val, x_test, y_train_plus_val, y_test = train_test_split(X, y, test_size=0.1, random_state=4)
# x_train, x_val, y_train, y_val = train_test_split(x_train_plus_val, y_train_plus_val, test_size=0.2, random_state=4)

# print(f" x_train: {x_train.shape},\n y_train: {y_train.shape},\n x_val: {x_val.shape},\n y_val: {y_val.shape},\n x_test: {x_test.shape},\n y_test: {y_test.shape}")

x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=4)
x_train.shape, x_test.shape, y_train.shape, y_test.shape

# # Reshape y
# y_train = np.reshape(y_train, (-1, 1))
# y_val = np.reshape(y_val, (-1, 1))
# y_test = np.reshape(y_test, (-1, 1))
# print(f" x_train: {x_train.shape},\n y_train: {y_train.shape},\n x_val: {x_val.shape},\n y_val: {y_val.shape},\n x_test: {x_test.shape},\n y_test: {y_test.shape}")

# Reshape y_train and y_test
y_train = np.reshape(y_train, (-1, 1))
y_test = np.reshape(y_test, (-1, 1))
x_train.shape, x_test.shape, y_train.shape, y_test.shape

from sklearn.preprocessing import MinMaxScaler, StandardScaler

# Initialize MinMaxScaler
scaler = MinMaxScaler()

scaler.fit(x_train)
x_train = scaler.transform(x_train)
x_test = scaler.transform(x_test)

"""## Part b"""

from keras.utils import to_categorical
from sklearn.preprocessing import LabelBinarizer

from sklearn.neural_network import MLPClassifier, MLPRegressor
from imblearn.under_sampling import RandomUnderSampler
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
import numpy as np

import tensorflow as tf
from tensorflow import keras
from keras import preprocessing
from keras.models import Sequential
from keras.layers import Dense

model_2 = Sequential()

# Add the first hidden layer with 50 neurons and linear activation function
model_2.add(Dense(15, activation='relu', input_shape=(x_train.shape[1],)))

# Add the second hidden layer with 30 neurons and linear activation function
model_2.add(Dense(8, activation='relu'))

# Add an output layer with 1 neuron and linear activation function
model_2.add(Dense(4, activation='softmax'))

model_2.summary()

model_2.compile(optimizer='adam', loss='SparseCategoricalCrossentropy', metrics=['accuracy'])
history = model_2.fit(x_train, y_train, validation_split=0.2, epochs=100 ,batch_size=10)

#Evaluate the model
loss = model_2.evaluate(x_test , y_test)

y_pred_2 = model_2.predict(x_test)

from sklearn.metrics import accuracy_score

# Assuming y_test contains the true labels
y_pred_classes = np.argmax(y_pred_2, axis=1)
accuracy = accuracy_score(y_test, y_pred_classes)
print("Accuracy:", accuracy)

history.history.keys()

import matplotlib.pyplot as plt

plt.plot(history.history['loss'], label='train')   # Training loss
plt.plot(history.history['val_loss'], label='val')  # Validation loss

plt.legend(['Training Loss', 'Validation Loss'])
plt.title('Loss over Epochs')
plt.xlabel("Epochs")
plt.ylabel("Loss")
plt.show()

# Plot accuracy
plt.plot(history.history['accuracy'], label='Training Accuracy')
plt.plot(history.history['val_accuracy'], label='Validation Accuracy')
plt.xlabel('Epochs')
plt.ylabel('Accuracy')
plt.title('Accuracy over Epochs')
plt.legend()

plt.show()

y_test.shape

y_pred_classes = np.argmax(y_pred_2, axis=1)  # Convert predicted probabilities to class labels

from sklearn.metrics import classification_report

print(classification_report(y_test, y_pred_classes))

import numpy as np
import matplotlib.pyplot as plt
from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay

y_pred_classes = np.argmax(y_pred_2, axis=1)  # Convert predicted probabilities to class labels

cm = confusion_matrix(y_test, y_pred_classes)

target_names = ['Normal', 'IR007', 'BB007', 'OR007']

disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=target_names)
disp.plot(cmap=plt.cm.Blues)
plt.title('Confusion Matrix')
plt.show()

"""## Part c

### SGD & Hinge
"""

model_2_2 = Sequential()

# Add the first hidden layer with 50 neurons and linear activation function
model_2_2.add(Dense(15, activation='relu', input_shape=(x_train.shape[1],)))

# Add the second hidden layer with 30 neurons and linear activation function
model_2_2.add(Dense(8, activation='relu'))

# Add an output layer with 1 neuron and linear activation function
model_2_2.add(Dense(4, activation='softmax'))

model_2_2.summary()

from tensorflow.keras.losses import KLDivergence

model_2_2.compile(optimizer='SGD', loss='SparseCategoricalCrossentropy', metrics=['accuracy'])
history_SGD_Hinge = model_2_2.fit(x_train, y_train, validation_split=0.2, epochs=100 ,batch_size=10)

history = history_SGD_Hinge

history.history.keys()

#Evaluate the model
loss = model_2_2.evaluate(x_test , y_test)

y_pred_2_2 = model_2_2.predict(x_test)


from sklearn.metrics import accuracy_score

# Assuming y_test contains the true labels
y_pred_classes = np.argmax(y_pred_2_2, axis=1)  # Convert predicted probabilities to class labels
accuracy = accuracy_score(y_test, y_pred_classes)
print("Accuracy:", accuracy)

import matplotlib.pyplot as plt

plt.plot(history.history['loss'], label='train')   # Training loss
plt.plot(history.history['val_loss'], label='val')  # Validation loss

plt.legend(['Training Loss', 'Validation Loss'])
plt.xlabel("Epochs")
plt.ylabel("Loss")
plt.show()

# Plot accuracy
plt.plot(history.history['accuracy'], label='Training Accuracy')
plt.plot(history.history['val_accuracy'], label='Validation Accuracy')
plt.xlabel('Epochs')
plt.ylabel('Accuracy')
plt.title('Accuracy over Epochs')
plt.legend()

plt.show()

from sklearn.metrics import classification_report

print(classification_report(y_test, y_pred_classes))

import numpy as np
import matplotlib.pyplot as plt
from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay

y_pred_classes = np.argmax(y_pred_2_2, axis=1)  # Convert predicted probabilities to class labels

cm = confusion_matrix(y_test, y_pred_classes)

target_names = ['Normal', 'IR007', 'BB007', 'OR007']

disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=target_names)
disp.plot(cmap=plt.cm.Blues)
plt.title('Confusion Matrix')
plt.show()

"""## Part d"""

from sklearn.model_selection import KFold

kf = KFold(n_splits=5, shuffle=True, random_state=4)
accuracies = []

def build_model():
    model_2_3 = Sequential()
    model_2_3.add(Dense(15, activation='relu', input_shape=(x_train.shape[1],)))
    model_2_3.add(Dense(8, activation='relu'))
    model_2_3.add(Dense(4, activation='softmax'))
    model_2_3.compile(optimizer='adam', loss='SparseCategoricalCrossentropy', metrics=['accuracy'])
    return model_2_3

for train_index, valid_index in kf.split(x_train):
    # Split the data
    x_train_k, x_valid_k = x_train[train_index], x_train[valid_index]
    y_train_k, y_valid_k = y_train[train_index], y_train[valid_index]

    # Initialize and train the model (RandomForestClassifier in this case)
    # model = RandomForestClassifier(n_estimators=100, random_state=42)
    model_2_3 = build_model()
    model_2_3.fit(x_train_k, y_train_k, validation_data=(x_valid_k, y_valid_k), epochs=100, batch_size=10)


    # Predict on the test set
    y_pred_2_3 = model_2_3.predict(x_valid_k)
    y_pred_classes = np.argmax(y_pred_2_3, axis=1)


    cm = confusion_matrix(y_valid_k, y_pred_classes)

    target_names = ['Normal', 'IR007', 'BB007', 'OR007']

    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=target_names)
    disp.plot(cmap=plt.cm.Blues)
    plt.title('Confusion Matrix')
    plt.show()


    accuracy = accuracy_score(y_valid_k, y_pred_classes)

    accuracies.append(accuracy)

# Calculate the average accuracy
average_accuracy = np.mean(accuracies)
print(f'Average Accuracy: {average_accuracy:.2f}')

# Predict on the test set
    y_pred_2_3_test = model_2_3.predict(x_test)
    y_pred_classes_test = np.argmax(y_pred_2_3_test, axis=1)


    cm = confusion_matrix(y_test, y_pred_classes_test)

    target_names = ['Normal', 'IR007', 'BB007', 'OR007']

    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=target_names)
    disp.plot(cmap=plt.cm.Blues)
    plt.title('Confusion Matrix')
    plt.show()

"""# Problem 3"""

cov_type = fetch_covtype()

cov_type.data.shape

cov_type.target.shape

cov_type.feature_names[:4]

num_samples = 1000

total_samples = cov_type.data.shape[0]

random_indices = np.random.choice(total_samples, size=num_samples, replace=False)

X = cov_type.data[random_indices]
y = cov_type.target[random_indices]

X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=4)
X_train.shape, X_test.shape

unique_values = np.unique(y)
print(unique_values)

clf = tree.DecisionTreeClassifier(random_state=4, ccp_alpha=0.005)
clf.fit(X_train, y_train)

tree.plot_tree(clf)

print(y_test)

clf.predict(X_test)

clf.score(X_test, y_test)













"""## Part a"""

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.datasets import fetch_covtype
from sklearn import tree
import numpy as np
import matplotlib.pyplot as plt
from sklearn.utils import resample

!gdown 1XI-a6XbUk23bh0dNirGN8-drYX1ZUtFj

drug_data = pd.read_csv("/content/drug200.csv")

drug_data.head()

drug_data.info()

column_name = 'Drug'
num_unique_values = drug_data[column_name].nunique()
value_counts = drug_data[column_name].value_counts()
print(f'The number of unique values in the column "{column_name}" is {num_unique_values}')
print(value_counts)

# Plotting the value counts
plt.figure(figsize=(5, 3))
value_counts.plot(kind='bar')
plt.title(f'Counts of Each Unique Value in Column "{column_name}"')
plt.xlabel('Drug Type')
plt.ylabel('Count')
plt.xticks(rotation=45)
plt.show()

df_drugY = drug_data[drug_data[column_name] == 'drugY']
df_drugX = drug_data[drug_data[column_name] == 'drugX']
df_drugA = drug_data[drug_data[column_name] == 'drugA']
df_drugC = drug_data[drug_data[column_name] == 'drugC']
df_drugB = drug_data[drug_data[column_name] == 'drugB']


# Undersample the majority class
df_drugY_undersampled = resample(df_drugY,
                                    replace=False,    # sample without replacement
                                    n_samples=16,  # to match the minority classes
                                    random_state=123) # reproducible results

df_drugX_undersampled = resample(df_drugX,
                                    replace=False,    # sample without replacement
                                    n_samples=16,  # to match the minority classes
                                    random_state=123) # reproducible results

df_drugA_undersampled = resample(df_drugA,
                                    replace=False,    # sample without replacement
                                    n_samples=16,  # to match the minority classes
                                    random_state=123) # reproducible results



# Combine undersampled majority class with minority classes
df_undersampled = pd.concat([df_majority_undersampled, df_minority_X, df_minority_A, df_minority_C, df_minority_B])

# Display new class counts
print(df_undersampled[column_name].value_counts())

unique_values_Sex = np.unique(drug_data["Sex"])
unique_values_BP = np.unique(drug_data["BP"])
unique_values_Cholesterol = np.unique(drug_data["Cholesterol"])

print("unique_values_Sex = " ,unique_values_Sex)
print("unique_values_BP = ", unique_values_BP)
print("unique_values_Cholesterol", unique_values_Cholesterol)

Sex_mapping = {'F': 1, 'M': 2}
BP_mapping = {'HIGH': 1, 'LOW': 2, 'NORMAL': 3}
Cholesterol_mapping = {'HIGH': 1, 'NORMAL': 2}

drug_data['Sex'] = drug_data['Sex'].replace(Sex_mapping)
drug_data['BP'] = drug_data['BP'].replace(BP_mapping)
drug_data['Cholesterol'] = drug_data['Cholesterol'].replace(Cholesterol_mapping)

drug_data.head()

X_drug = drug_data.drop(columns=['Drug']).values
y_drug = drug_data['Drug'].values

X_train_d, X_test_d, y_train_d, y_test_d = train_test_split(X_drug, y_drug, random_state=4, test_size=0.15)
X_train_d.shape, X_test_d.shape

clf = tree.DecisionTreeClassifier(random_state=4, ccp_alpha=0.1, criterion='entropy')
clf.fit(X_train_d, y_train_d)

tree.plot_tree(clf)

y_test_d

y_pred = clf.predict(X_test_d)
y_pred

clf.score(X_test_d, y_test_d)

"""##Part b"""

import numpy as np
import matplotlib.pyplot as plt
from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay

y_pred_classes = np.argmax(y_pred_2_2, axis=1)  # Convert predicted probabilities to class labels

cm = confusion_matrix(y_test_d, y_pred)

target_names = ['A', 'B', 'C', 'X', 'Y']

disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=target_names)
disp.plot(cmap=plt.cm.Blues)
plt.title('Confusion Matrix')
plt.show()

from sklearn.metrics import accuracy_score, precision_score, recall_score

accuracy = accuracy_score(y_test_d, y_pred)
precision = precision_score(y_test_d, y_pred, average='weighted')
recall = recall_score(y_test_d, y_pred, average='weighted')

print("Accuracy:", accuracy)
print("Precision:", precision)
print("Recall:", recall)

"""## Part c"""

from sklearn.ensemble import RandomForestClassifier
from sklearn.datasets import make_classification
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.metrics import accuracy_score

param_grid = {
    'n_estimators': [50, 100, 150],
    'max_depth': [4, 6, 8, None],
    'min_samples_split': [2, 5, 10],
    'min_samples_leaf': [1, 2, 4],
    'bootstrap': [True, False]
}

rf_classifier = RandomForestClassifier(random_state=4)

grid_search = GridSearchCV(estimator=rf_classifier, param_grid=param_grid, cv=5, scoring='accuracy')

grid_search.fit(X_train_d, y_train_d)

# Get the best hyperparameters
best_params = grid_search.best_params_
print("Best Hyperparameters:", best_params)

# Use the best model to predict on test data
best_model = grid_search.best_estimator_
y_pred = best_model.predict(X_test_d)

# Evaluate the classifier
accuracy = accuracy_score(y_test_d, y_pred)
print("Accuracy:", accuracy)

from sklearn.metrics import accuracy_score, precision_score, recall_score

accuracy = accuracy_score(y_test_d, y_pred)
precision = precision_score(y_test_d, y_pred, average='weighted')
recall = recall_score(y_test_d, y_pred, average='weighted')

print("Accuracy:", accuracy)
print("Precision:", precision)
print("Recall:", recall)

"""# Problem 4"""

# Commented out IPython magic to ensure Python compatibility.
from pandas import DataFrame as df
import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
# %matplotlib inline

!gdown 1qYg0L0-iDvhawJw-v5keUtQc9FpYALxB

data = pd.read_csv("/content/heart.csv")
data.head()

data.info()

data.describe()

data.groupby('target').size()

sns.countplot(x='target', data=data)

sns.pairplot(data,hue='target')

data.hist()

correlation_matrix = data.corr()

plt.figure(figsize=(10, 8))
sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt=".2f")
plt.title('Correlation Matrix')
plt.show()

from sklearn.utils import shuffle
shuffled_data = shuffle(data, random_state=4)
print("shuffled_data shape = ",shuffled_data.shape)

X=np.array(shuffled_data.loc[:,data.columns!='target'])
y=np.array(shuffled_data.loc[:,data.columns=='target'])
print(X.shape, y.shape)

X

"""## Balance data"""

from imblearn.under_sampling import RandomUnderSampler

# rus = RandomUnderSampler(random_state=4)
rus = RandomUnderSampler(sampling_strategy={0: 499, 1: 499}, random_state=4)

# Resample the dataset
UnderSample = False
if UnderSample :
  X_resampled, y_resampled = rus.fit_resample(X, y)
  print("Class Distribution after Random Undersampling:")
  print(pd.Series(y_resampled).value_counts())
else :
  X_resampled = X
  y_resampled = y

sns.countplot(x='target', data=pd.Series(y_resampled))

from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(X_resampled, y_resampled, test_size=0.2, random_state=4)
print('train:', X_train.shape, y_train.shape, '\ntest: ', X_test.shape, y_test.shape)

print(y_train.shape)
print(y_train.ravel().shape)

from sklearn.preprocessing import StandardScaler, MinMaxScaler

scaler = MinMaxScaler()
scaler.fit(X_train)
print(X_train[:5])
X_train_t = scaler.transform(X_train)
X_test_t = scaler.transform(X_test)
print(X_train_t[:5])

"""## NaiveBayes Classifier"""

class NaiveBayes:
    def fit(self, X, y):
        n_samples, n_features = X.shape
        self._classes = np.unique(y)
        n_classes = len(self._classes)

        # calculate mean, var, and prior for each class
        self._mean = np.zeros((n_classes, n_features), dtype=np.float64)
        self._var = np.zeros((n_classes, n_features), dtype=np.float64)
        self._priors = np.zeros(n_classes, dtype=np.float64)

        for idx, c in enumerate(self._classes):
            X_c = X[y == c]
            self._mean[idx, :] = X_c.mean(axis=0)
            self._var[idx, :] = X_c.var(axis=0)
            self._priors[idx] = X_c.shape[0] / float(n_samples)


    def predict(self, X):
        y_pred = [self._predict(x) for x in X]
        return np.array(y_pred)

    def _predict(self, x):
        posteriors = []

        # calculate posterior probability for each class
        for idx, c in enumerate(self._classes):
            prior = np.log(self._priors[idx])
            posterior = np.sum(np.log(self._pdf(idx, x)))
            posterior = posterior + prior
            posteriors.append(posterior)

        # return class with the highest posterior
        return self._classes[np.argmax(posteriors)]

    def _pdf(self, class_idx, x):
        mean = self._mean[class_idx]
        var = self._var[class_idx]
        numerator = np.exp(-((x - mean) ** 2) / (2 * var))
        denominator = np.sqrt(2 * np.pi * var)
        return numerator / denominator

from sklearn.naive_bayes import GaussianNB
MyNB = NaiveBayes()
SKNB = GaussianNB()

MyNB.fit(X_train, y_train.ravel())
SKNB.fit(X_train, y_train.ravel())
pred = MyNB.predict(X_test)
pred2 = SKNB.predict(X_test)

MyNB2 = NaiveBayes()
SKNB2 = GaussianNB()
MyNB2.fit(X_train_t, y_train.ravel())
SKNB2.fit(X_train_t, y_train.ravel())
pred_t = MyNB2.predict(X_test_t)
pred2_t = SKNB2.predict(X_test_t)

"""## Evaluation

"""

from sklearn.metrics import classification_report
print(classification_report(y_test,pred))

from sklearn.metrics import classification_report
print(classification_report(y_test,pred_t))

print(pred)
print(pred_t)

from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay
cm = confusion_matrix(y_test,pred)
names = list(data.groupby('target').groups.keys())
disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=names)
disp.plot()
plt.show()

from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, jaccard_score

print('Accuracy :',accuracy_score(y_test,pred))
print('Precision :',precision_score(y_test,pred,average='micro'))
print('Recall :',recall_score(y_test,pred,average='micro'))
print('F1 score :',f1_score(y_test,pred,average='micro'))
print('Jaccard score :',jaccard_score(y_test,pred,average='micro'))

from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, jaccard_score

print('Accuracy :',accuracy_score(y_test,pred_t))
print('Precision :',precision_score(y_test,pred_t,average='micro'))
print('Recall :',recall_score(y_test,pred_t,average='micro'))
print('F1 score :',f1_score(y_test,pred_t,average='micro'))
print('Jaccard score :',jaccard_score(y_test,pred_t,average='micro'))

np.random.seed(4)
random_data = np.random.randint(0, len(X_test), 5)
print(random_data)
five_data = X_test[random_data]
five_data.shape

pred_t_five_data = MyNB.predict(five_data)
true_lable = y_test[random_data].T
print('pred:        ',pred_t_five_data)
print('true lable: ', true_lable)